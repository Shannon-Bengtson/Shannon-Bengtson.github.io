{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the most contains the latest version of the time dependent BN modeling for Tarawa, Kiribati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:06.245034Z",
     "start_time": "2021-06-10T02:18:06.200060Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:09.215577Z",
     "start_time": "2021-06-10T02:18:06.246033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-leaflet/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, time, timedelta\n",
    "import pysmile\n",
    "import pysmile_license\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('/src/python_classes')\n",
    "import rpy2\n",
    "# os.environ['R_HOME'] = 'C:\\ProgramData\\Anaconda3\\Lib\\R'\n",
    "# %load_ext rpy2.ipython\n",
    "!jupyter nbextension enable --py --sys-prefix ipyleaflet\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipyleaflet import *\n",
    "import ipywidgets as widgets\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import geojson\n",
    "import folium\n",
    "from folium.plugins import FloatImage as FloatImage\n",
    "from colormap import rgb2hex\n",
    "import rpy2\n",
    "os.environ['R_HOME'] = '/lib/R'\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "from BNModel import BNModel\n",
    "\n",
    "from preprocessing_all_points import *\n",
    "from preprocessing_points_spatially_temporally import *\n",
    "from compile_model_t import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:09.301509Z",
     "start_time": "2021-06-10T02:18:09.217559Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### set location of file storage\n",
    "# folder = 'BN_antonio_data'\n",
    "# try:\n",
    "#     os.makedirs(folder)\n",
    "# except FileExistsError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:16.558430Z",
     "start_time": "2021-06-10T02:18:09.303508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import and preprocess data\n",
    "df_lagoon_profiles,df_ocean_profiles,inundation_dict,winds_dict,waves_dict,tide_dict,sla_dict,time_dict = \\\n",
    "    loading_tarawa_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Variable Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:00.324519Z",
     "start_time": "2021-06-10T02:19:00.237568Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def initialise_model_dictionaries():\n",
    "    #### Don't include spaces in bin names. if no discretisation, just leave out that key\n",
    "    lagoon_model_dict = {\n",
    "        'variables':{\n",
    "            'wind_u':{\n",
    "                'label':'Wind u vector (m/s)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'wind_v':{\n",
    "                'label':r'Wind v vector (m/s)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'uniform',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'Hs_offshore':{\n",
    "                'label':'Offshore wave height (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','HighMid','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'Tm_offshore':{\n",
    "                'label':'Offshore wave period (s)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'Dir_offshore':{\n",
    "                'label':r'Offshore wave direction (degrees)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':8,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['NNE','ENE','ESE','SSE','SSW','WSW','WNW','NNW']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'WL_wave_comp':{\n",
    "                'label':'Water level from wave component (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':[]\n",
    "            }\n",
    "        },\n",
    "        'training_frac':0.8,\n",
    "        'bootstrap_reps':1\n",
    "    }\n",
    "    ocean_model_dict = {\n",
    "       'variables':{\n",
    "           'Tm_offshore':{\n",
    "                'label':'Wave period offshore (?)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'Hs_offshore':{\n",
    "                'label':'Wave height offshore (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'Dir_offshore':{\n",
    "                'label':'Wave direction offshore (degrees)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':8,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['NNE','ENE','ESE','SSE','SSW','WSW','WNW','NNW']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'WL_wave_comp':{\n",
    "                'label':'Water level from wave component (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':[]\n",
    "            },\n",
    "            'reef_width':{\n",
    "                'label':'Reef width (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'reef_depth':{\n",
    "                'label':'Reef depth (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'forereef_slope':{\n",
    "                'label':'Fore reef slope (degrees)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            },\n",
    "            'shore_dir':{\n",
    "                'label':'Shoreline direction (degrees)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':3,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['NE','S','NW']\n",
    "                },\n",
    "                'child_nodes':['WL_wave_comp']\n",
    "            }\n",
    "       },\n",
    "        'training_frac':0.8,\n",
    "        'bootstrap_reps':1\n",
    "    }\n",
    "    return(lagoon_model_dict,ocean_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_locator(value,bin_edges):\n",
    "    '''\n",
    "    function used for determining the index of the appropriate bin for a numerical value.\n",
    "    '''\n",
    "    i=0\n",
    "    for edge_1,edge_2 in zip(bin_edges[:-1],bin_edges[1:]):\n",
    "        if (value>edge_1)&(value<=edge_2):\n",
    "            loc_bin = i\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    if value<=bin_edges[0]:\n",
    "        loc_bin = 0\n",
    "\n",
    "    if value>=bin_edges[-1]:\n",
    "        loc_bin = len(bin_edges)-2\n",
    "\n",
    "    return(loc_bin)\n",
    "\n",
    "def model_location(model_dict,location_details,evidence_dict,variable_list):\n",
    "    \n",
    "    '''\n",
    "    function for adding the location information for one side model to the evidence dictionary\n",
    "    '''\n",
    "    \n",
    "    for variable in variable_list:\n",
    "\n",
    "        bin_edges = model_dict['variables'][variable]['bin_edges'][0]\n",
    "        value = location_details[variable]\n",
    "\n",
    "        var_bin = bin_locator(value,bin_edges)\n",
    "        \n",
    "        evidence_array = [0]*(len(bin_edges)-1)\n",
    "        evidence_array[var_bin] = 1\n",
    "        \n",
    "        evidence_dict.update({\n",
    "            variable:evidence_array\n",
    "        })\n",
    "\n",
    "    # Add evidence to model dict\n",
    "    model_location_dict = BNModel().add_evidence_to_dict(model_dict,evidence_dict)\n",
    "    \n",
    "    # Set evidence and get beliefs\n",
    "    model_location_dict = BNModel().update_evidence(model_location_dict)\n",
    "    \n",
    "    return(model_location_dict)\n",
    "\n",
    "def location_probabilities(evidence_dict,model_dict,variable_list,df_profiles):\n",
    "    '''\n",
    "    \n",
    "    Function for setting evidence and determing probabilties for twl at each point around the island based \n",
    "    on the reef characteristics at each location\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    location_probabilities_dict = {}\n",
    "    \n",
    "    for index,row in df_profiles.iterrows():\n",
    "\n",
    "        model_location_dict = model_location(model_dict,row,evidence_dict,variable_list)\n",
    "        location_probabilities = model_location_dict['variables']['TWL']['resulting_probs'][0]\n",
    "\n",
    "        df_location_probabilities = pd.DataFrame.from_dict(location_probabilities,orient='index')\n",
    "        \n",
    "        largest_cat = df_location_probabilities.idxmax()[0]\n",
    "\n",
    "        location_probabilities_dict.update({\n",
    "            (row.reef_long,row.reef_lat):\\\n",
    "                model_dict['variables']['TWL']['discretisation']['bin_names'].index(largest_cat)\n",
    "        })\n",
    "        \n",
    "    return(location_probabilities_dict)\n",
    "\n",
    "def add_time_lag(df,location_vars_list,time_delay_vars):\n",
    "    previous_time_dict = {}\n",
    "\n",
    "    for loc_vars,group in df.groupby(location_vars_list):\n",
    "        df_orig = group.reset_index(drop=True)\n",
    "        df_delay = group[time_delay_vars].iloc[:-1].reset_index(drop=True)\n",
    "        df_delay.index = [x+1 for x in list(df_delay.index)]\n",
    "        df_delay.drop('time',axis=1,inplace=True)\n",
    "        group = df_orig.join(df_delay,rsuffix='_t_1',how='inner')\n",
    "\n",
    "        previous_time_dict.update({\n",
    "            loc_vars:group\n",
    "        })\n",
    "\n",
    "    df = pd.concat(previous_time_dict)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One network per time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models created\n",
      "1993-01-01 00:00:00\n",
      "models created\n",
      "1993-01-02 00:00:00\n",
      "models created\n",
      "1993-01-03 00:00:00\n",
      "models created\n",
      "1993-01-04 00:00:00\n"
     ]
    }
   ],
   "source": [
    "model_dicts_through_time_dict = {}\n",
    "\n",
    "time_min = inundation_dict['Time'][0]\n",
    "time_step = timedelta(hours=24)\n",
    "max_steps = 7\n",
    "\n",
    "times_for_model = []\n",
    "\n",
    "for step in np.arange(0,max_steps,1):\n",
    "    time_at_step = time_min+time_step*step\n",
    "    \n",
    "    times_for_model.append(time_at_step)\n",
    "    \n",
    "times_for_model = times_for_model[:10]\n",
    "\n",
    "lagoon_probabilities_dict = {}\n",
    "ocean_probabilities_dict = {}\n",
    "    \n",
    "for time_min,time_max in zip(times_for_model[:-2],times_for_model[2:]):\n",
    "    lower_idx = np.abs([x-time_min for x in inundation_dict['Time']]).argmin()\n",
    "    upper_idx = np.abs([x-time_max for x in inundation_dict['Time']]).argmin()\n",
    "       \n",
    "    inundation_time_slice_dict = {var:inundation_dict[var][lower_idx:upper_idx,:] for var in ['TWL','Tide']}\n",
    "    inundation_time_slice_dict['Time'] = inundation_dict['Time'][lower_idx:upper_idx]\n",
    "    inundation_time_slice_dict['Ptos'] = inundation_dict['Ptos']\n",
    "    winds_time_slice_dict = {\n",
    "        'wind_u':winds_dict['wind_u'][lower_idx:upper_idx],'wind_v':winds_dict['wind_v'][lower_idx:upper_idx]}\n",
    "    waves_time_slice_dict = {var:waves_dict[var][lower_idx:upper_idx,:] for var in ['Diro','Hso','Tmo','Tpo']}\n",
    "    waves_time_slice_dict['Timeo'] = waves_dict['Timeo'][lower_idx:upper_idx]\n",
    "    \n",
    "    tide_time_slice_dict = {'Tide':tide_dict['Tide'][lower_idx:upper_idx]}\n",
    "    sla_time_slice_dict = {'MSL':sla_dict['MSL'][lower_idx:upper_idx]}\n",
    "    time_slice_dict = {'time':time_dict['time'][lower_idx:upper_idx]}\n",
    "    \n",
    "    # list of variable that are predicted and included in lag\n",
    "    predicted_lag_variables = []\n",
    "    \n",
    "    # Initialise model dicts\n",
    "    lagoon_model_dict,ocean_model_dict = initialise_model_dictionaries()\n",
    "\n",
    "    df_ocean,df_lagoon = \\\n",
    "        preprocessing_points_spatially_temporally(df_lagoon_profiles,\n",
    "                                                  df_ocean_profiles,\n",
    "                                                  inundation_time_slice_dict,\n",
    "                                                  winds_time_slice_dict,\n",
    "                                                  waves_time_slice_dict,\n",
    "                                                  tide_time_slice_dict,\n",
    "                                                  sla_time_slice_dict,\n",
    "                                                  time_slice_dict,\n",
    "                                                  predicted_lag_variables)\n",
    "    \n",
    "    # Define which variables are to be time lagged\n",
    "    ocean_time_delay_vars = ['time','WL_wave_comp']\n",
    "    lagoon_time_delay_vars = ['time','WL_wave_comp','wind_u','wind_v','Dir_offshore','Hs_offshore','Tm_offshore']\n",
    "\n",
    "    # Because the lag is going to depend on the location, define the list of variables which are used to group by loc\n",
    "    location_vars_list = ['lat','long']\n",
    "\n",
    "    # Add the time lag variables\n",
    "    df_ocean_with_lag = add_time_lag(df_ocean,location_vars_list,ocean_time_delay_vars)\n",
    "    df_lagoon_with_lag = add_time_lag(df_lagoon,location_vars_list,lagoon_time_delay_vars)\n",
    "\n",
    "    # Create dictionary of variables\n",
    "    lagoon_data_dict = {column:np.array(df_lagoon_with_lag[column]) for column in df_lagoon_with_lag.columns}\n",
    "    ocean_data_dict = {column:np.array(df_ocean_with_lag[column]) for column in df_ocean_with_lag.columns}\n",
    "\n",
    "    # Remove the variables that I don't want to include in the model\n",
    "    lagoon_data_dict = {key:item for key,item in lagoon_data_dict.items() if key not in ['lat','long','time','TWL','Tide','MSL']}\n",
    "    ocean_data_dict = {key:item for key,item in ocean_data_dict.items() if key not in ['lat','long','time','wind_u','wind_v','TWL','Tide','MSL']}\n",
    "\n",
    "    # Duplicate the variables to make T-1 in the model dictionary\n",
    "    for var in ocean_time_delay_vars:\n",
    "        if var!='time':\n",
    "            ocean_model_dict['variables'][var+'_t_1'] = ocean_model_dict['variables'][var].copy()\n",
    "            ocean_model_dict['variables'][var+'_t_1'].update({\n",
    "                'child_nodes':[var]\n",
    "            })\n",
    "    for var in lagoon_time_delay_vars:\n",
    "        if var!='time':\n",
    "            lagoon_model_dict['variables'][var+'_t_1'] = lagoon_model_dict['variables'][var].copy()\n",
    "            lagoon_model_dict['variables'][var+'_t_1'].update({\n",
    "                'child_nodes':[var]\n",
    "            })\n",
    "    \n",
    "    # remove lat long as indices\n",
    "    df_ocean_with_lag = df_ocean_with_lag.reset_index(drop=True)\n",
    "    df_lagoon_with_lag = df_lagoon_with_lag.reset_index(drop=True)\n",
    "    \n",
    "    lagoon_model_dict,ocean_model_dict = create_BN_time_t(lagoon_model_dict,\n",
    "                                                          lagoon_data_dict,\n",
    "                                                          df_lagoon_with_lag,\n",
    "                                                          ocean_model_dict,\n",
    "                                                          df_ocean_with_lag,\n",
    "                                                          ocean_data_dict)    \n",
    "    \n",
    "    variable_list = ['reef_width','reef_depth','forereef_slope','shore_dir']\n",
    "\n",
    "    # Create an empty location probability dictionary if this is the first timestep\n",
    "    location_probabilities_dict = {}\n",
    "\n",
    "    # Loop over each location, and create dictionary of each of the reef characteristics\n",
    "    for index,row in df_ocean_profiles.iterrows():\n",
    "\n",
    "        evidence_dict = {}\n",
    "\n",
    "        for variable in variable_list:\n",
    "\n",
    "            bin_edges = ocean_model_dict['variables'][variable]['bin_edges'][0]\n",
    "            value = row[variable]\n",
    "\n",
    "            var_bin = bin_locator(value,bin_edges)\n",
    "\n",
    "            evidence_array = [0]*(len(bin_edges)-1)\n",
    "            evidence_array[var_bin] = 1\n",
    "\n",
    "            evidence_dict.update({\n",
    "                variable:evidence_array\n",
    "            })\n",
    "\n",
    "        # Add evidence to model dict\n",
    "        ocean_model_location_dict = BNModel().add_evidence_to_dict(ocean_model_dict,evidence_dict)\n",
    "\n",
    "        # Set evidence and get beliefs\n",
    "        ocean_model_dict = BNModel().update_evidence(ocean_model_dict\n",
    "                                                    )\n",
    "        # get location probs and put into a dict\n",
    "        location_probabilities = ocean_model_location_dict['variables']['WL_wave_comp']['resulting_probs'][0]\n",
    "        location_probabilities_dict.update({\n",
    "            (row.reef_long,row.reef_lat):[x for y,x in location_probabilities.items()]\n",
    "        })\n",
    "        \n",
    "    ocean_probabilities_dict.update({\n",
    "        time_min:location_probabilities_dict\n",
    "    })\n",
    "\n",
    "    variable_list = []\n",
    "\n",
    "    # Create an empty location probability dictionary if this is the first timestep\n",
    "    location_probabilities_dict = {}\n",
    "\n",
    "    # Loop over each location, and create dictionary of each of the reef characteristics\n",
    "    for index,row in df_lagoon_profiles.iterrows():\n",
    "\n",
    "        evidence_dict = {}\n",
    "\n",
    "        for variable in variable_list:\n",
    "\n",
    "            bin_edges = lagoon_model_dict['variables'][variable]['bin_edges'][0]\n",
    "            value = row[variable]\n",
    "\n",
    "            var_bin = bin_locator(value,bin_edges)\n",
    "\n",
    "            evidence_array = [0]*(len(bin_edges)-1)\n",
    "            evidence_array[var_bin] = 1\n",
    "\n",
    "            evidence_dict.update({\n",
    "                variable:evidence_array\n",
    "            })\n",
    "\n",
    "        # Add evidence to model dict\n",
    "        lagoon_model_location_dict = BNModel().add_evidence_to_dict(lagoon_model_dict,evidence_dict)\n",
    "\n",
    "        # Set evidence and get beliefs\n",
    "        lagoon_model_dict = BNModel().update_evidence(lagoon_model_dict\n",
    "                                                    )\n",
    "        # get location probs and put into a dict\n",
    "        location_probabilities = lagoon_model_location_dict['variables']['WL_wave_comp']['resulting_probs'][0]\n",
    "        location_probabilities_dict.update({\n",
    "            (row.reef_long,row.reef_lat):[x for y,x in location_probabilities.items()]\n",
    "        })\n",
    "    \n",
    "    lagoon_probabilities_dict.update({\n",
    "        time_min:location_probabilities_dict\n",
    "    })\n",
    "    \n",
    "    print(time_min)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes from above: I have decided that the lag won't include prediction. I have now added in the lag variable into the lagoon and the ocean dataframe and the dictionary\n",
    "# Now I just need to troubleshoot the consequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which variables are to be time lagged\n",
    "ocean_time_delay_vars = ['time','WL_wave_comp']\n",
    "lagoon_time_delay_vars = ['time','WL_wave_comp','wind_u','wind_v','Dir_offshore','Hs_offshore','Tm_offshore']\n",
    "\n",
    "# Because the lag is going to depend on the location, define the list of variables which are used to group by loc\n",
    "location_vars_list = ['lat','long']\n",
    "\n",
    "# Add the time lag variables\n",
    "df_ocean_with_lag = add_time_lag(df_ocean,location_vars_list,ocean_time_delay_vars)\n",
    "df_lagoon_with_lag = add_time_lag(df_lagoon,location_vars_list,lagoon_time_delay_vars)\n",
    "\n",
    "# Create dictionary of variables\n",
    "lagoon_data_dict = {column:np.array(df_lagoon_with_lag[column]) for column in df_lagoon_with_lag.columns}\n",
    "ocean_data_dict = {column:np.array(df_ocean_with_lag[column]) for column in df_ocean_with_lag.columns}\n",
    "\n",
    "# # Remove the variables that I don't want to include in the model\n",
    "# lagoon_data_dict = {key:item for key,item in lagoon_data_dict.items() if key not in ['lat','long','time','TWL','Tide','MSL']}\n",
    "# ocean_data_dict = {key:item for key,item in ocean_data_dict.items() if key not in ['lat','long','time','wind_u','wind_v','TWL','Tide','MSL']}\n",
    "\n",
    "# # Duplicate the variables to make T-1 in the model dictionary\n",
    "# for var in ocean_time_delay_vars:\n",
    "#     if var!='time':\n",
    "#         ocean_model_dict['variables'][var+'_t_1'] = ocean_model_dict['variables'][var].copy()\n",
    "# for var in lagoon_time_delay_vars:\n",
    "#     if var!='time':\n",
    "#         lagoon_model_dict['variables'][var+'_t_1'] = lagoon_model_dict['variables'][var].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagoon_with_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate the variables to make T-1 in the model dictionary\n",
    "for var in ocean_time_delay_vars:\n",
    "    if var!='time':\n",
    "        ocean_model_dict['variables'][var+'_t_1'] = ocean_model_dict['variables'][var].copy()\n",
    "for var in lagoon_time_delay_vars:\n",
    "    if var!='time':\n",
    "        lagoon_model_dict['variables'][var+'_t_1'] = lagoon_model_dict['variables'][var].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagoon_model_dict['variables'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variable_list = ['reef_width','reef_depth','forereef_slope','shore_dir']\n",
    "\n",
    "# Create an empty location probability dictionary if this is the first timestep\n",
    "if time_min==times_for_model[0]:\n",
    "    location_probabilities_dict = {}\n",
    "\n",
    "# Loop over each location, and create dictionary of each of the reef characteristics\n",
    "for index,row in df_ocean_profiles.iterrows():\n",
    "\n",
    "    evidence_dict = {}\n",
    "\n",
    "    for variable in variable_list:\n",
    "\n",
    "        bin_edges = ocean_model_dict['variables'][variable]['bin_edges'][0]\n",
    "        value = row[variable]\n",
    "\n",
    "        var_bin = bin_locator(value,bin_edges)\n",
    "        \n",
    "        evidence_array = [0]*(len(bin_edges)-1)\n",
    "        evidence_array[var_bin] = 1\n",
    "        \n",
    "        evidence_dict.update({\n",
    "            variable:evidence_array\n",
    "        })\n",
    "        \n",
    "    # Now add previous TWL probabilities to dict\n",
    "    if time_min!=times_for_model[0]:\n",
    "        # have to add a new node for TWL t-1\n",
    "        predicted_lag_variables = ['TWL_t_1']\n",
    "        for var in predicted_lag_variables:\n",
    "            df_ocean_profiles[var] = 0#[item for key,item in ocean_probabilities_dict.items()]\n",
    "            \n",
    "        ocean_model_dict['variables'].update({\n",
    "                'TWL_t_1':{\n",
    "                'label':'Total water level bin at t-1 (1-5?)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['1','2','3','4','5']\n",
    "                },\n",
    "                'child_nodes':['TWL']\n",
    "            }\n",
    "        })\n",
    "        # Add twl at t-1 to evidence dict\n",
    "        evidence_dict.update({\n",
    "            'TWL_t_1':location_probabilities_dict[(row.reef_long,row.reef_lat)]\n",
    "\n",
    "    # Add evidence to model dict\n",
    "    ocean_model_location_dict = BNModel().add_evidence_to_dict(ocean_model_dict,evidence_dict)\n",
    "    \n",
    "    location_probabilities = ocean_model_location_dict['variables']['TWL']['resulting_probs'][0]\n",
    "\n",
    "    location_probabilities_dict.update({\n",
    "        (row.reef_long,row.reef_lat):[x for y,x in location_probabilities.items()]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_model_dict['variables']['TWL']['discretisation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_probabilities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location_dict['variables']['Tm_offshore'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_model_dict['variables'][''.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_probs_lagoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary for the evidence and populate as you go\n",
    "ocean_evidence_dict = {}\n",
    "\n",
    "# Create a list of variables that are location specific to set as evidence in the network\n",
    "\n",
    "\n",
    "# get the probability dictionary ### need to adjust so that it's not just the most likely, but the full distribution...\n",
    "location_probabilities_dict = location_probabilities(ocean_evidence_dict,ocean_model_dict,variable_list,df_ocean_profiles)\n",
    "\n",
    "# Create dataframe to plot\n",
    "df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# data_ocean = data2geojson(df_twl_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_probabilities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_probabilities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(location_probabilities_dict,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagoon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining times of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty dictionary to have a look at how the resulting probabilties varied through time\n",
    "# resulting_probs_dict = {}\n",
    "\n",
    "# for time,model_dict['lagoon'] in model_dicts_through_time_dict.items():\n",
    "    \n",
    "#     #Create an empty dictionary of evidence for now\n",
    "#     evidence_dict = {}\n",
    "    \n",
    "#     # Add evidence to model dict\n",
    "#     model_location_dict = BNModel().add_evidence_to_dict(model_dict,evidence_dict)\n",
    "\n",
    "#     # Set evidence and get beliefs\n",
    "#     model_location_dict = BNModel().update_evidence(model_location_dict)\n",
    "    \n",
    "#     # Get the resulting TWL probabilities (right now for a generic location)\n",
    "#     resulting_probs = model_location_dict['variables']['TWL']['resulting_probs']\n",
    "    \n",
    "#     # Add the resulting probabilities to a dictionary\n",
    "#     resulting_probs_dict.update({\n",
    "#         time:resulting_probs[0]\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting_probs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load in the reef and shoreline profile information\n",
    "# df_ocean_profiles = pd.read_csv('/src/Dataset/D8_tarawa_inundation/Profiles_definition_outer_reef_xyxy_processed.txt')\n",
    "# df_lagoon_profiles = pd.read_csv('/src/Dataset/D8_tarawa_inundation/Profiles_definition_inner_lagoon_xyxy.txt',delim_whitespace=True,header=None)\n",
    "# df_lagoon_profiles.columns = ['reef_long','reef_lat','shore_long','shore_lat','reef_depth']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bin_locator(value,bin_edges):\n",
    "#     '''\n",
    "#     function used for determining the index of the appropriate bin for a numerical value.\n",
    "#     '''\n",
    "#     i=0\n",
    "#     for edge_1,edge_2 in zip(bin_edges[:-1],bin_edges[1:]):\n",
    "#         if (value>edge_1)&(value<=edge_2):\n",
    "#             loc_bin = i\n",
    "#         else:\n",
    "#             i+=1\n",
    "#             continue\n",
    "\n",
    "#     if value<=bin_edges[0]:\n",
    "#         loc_bin = 0\n",
    "\n",
    "#     if value>=bin_edges[-1]:\n",
    "#         loc_bin = len(bin_edges)-2\n",
    "\n",
    "#     return(loc_bin)\n",
    "\n",
    "# def model_location(model_dict,location_details,evidence_dict,variable_list):\n",
    "    \n",
    "#     '''\n",
    "#     function for adding the location information for one side model to the evidence dictionary\n",
    "#     '''\n",
    "    \n",
    "#     for variable in variable_list:\n",
    "\n",
    "#         bin_edges = model_dict['variables'][variable]['bin_edges'][0]\n",
    "#         value = location_details[variable]\n",
    "\n",
    "#         var_bin = bin_locator(value,bin_edges)\n",
    "        \n",
    "#         evidence_array = [0]*(len(bin_edges)-1)\n",
    "#         evidence_array[var_bin] = 1\n",
    "        \n",
    "#         evidence_dict.update({\n",
    "#             variable:evidence_array\n",
    "#         })\n",
    "\n",
    "#     # Add evidence to model dict\n",
    "#     model_location_dict = BNModel().add_evidence_to_dict(model_dict,evidence_dict)\n",
    "    \n",
    "#     # Set evidence and get beliefs\n",
    "#     model_location_dict = BNModel().update_evidence(model_location_dict)\n",
    "    \n",
    "#     return(model_location_dict)\n",
    "\n",
    "# def location_probabilities(evidence_dict,model_dict,variable_list,df_profiles):\n",
    "#     '''\n",
    "    \n",
    "#     Function for setting evidence and determing probabilties for twl at each point around the island based \n",
    "#     on the reef characteristics at each location\n",
    "    \n",
    "#     '''\n",
    "    \n",
    "#     location_probabilities_dict = {}\n",
    "# #     figure_dict = {}\n",
    "    \n",
    "#     for index,row in df_profiles.iterrows():\n",
    "\n",
    "#         model_location_dict = model_location(model_dict,row,evidence_dict,variable_list)\n",
    "#         location_probabilities = model_location_dict['variables']['TWL']['resulting_probs'][0]\n",
    "\n",
    "#         df_location_probabilities = pd.DataFrame.from_dict(location_probabilities,orient='index')\n",
    "        \n",
    "# #         # Create figure for popup\n",
    "# #         fig = plt.figure(figsize=(2,2))\n",
    "# #         plt.bar(x=df_location_probabilities.index,height=df_location_probabilities[0])\n",
    "# #         plt.savefig('{}_{}.png'.format(int(row.reef_long*1000),int(row.reef_lat*1000)))\n",
    "# #         plt.close()\n",
    "        \n",
    "#         largest_cat = df_location_probabilities.idxmax()[0]\n",
    "\n",
    "#         location_probabilities_dict.update({\n",
    "#             (row.reef_long,row.reef_lat):\\\n",
    "#                 model_dict['variables']['TWL']['discretisation']['bin_names'].index(largest_cat)\n",
    "#         })\n",
    "        \n",
    "# #         figure_dict.update({\n",
    "# #             (row.reef_long,row.reef_lat):fig#html_graph\n",
    "# #         })\n",
    "        \n",
    "#     return(location_probabilities_dict)\n",
    "\n",
    "# def data2geojson(df):\n",
    "#     features = []\n",
    "#     insert_features = lambda X: features.append(\n",
    "#             geojson.Feature(geometry=geojson.Point((X[\"long\"],\n",
    "#                                                     X[\"lat\"])),\n",
    "#                             properties=dict(name=X[\"most_likely_twl\"])))\n",
    "#     df.apply(insert_features, axis=1)\n",
    "        \n",
    "#     return(geojson.FeatureCollection(features))\n",
    "\n",
    "# # Load SLR Projections\n",
    "# data_location = \"/src/Dataset/D7_MSL_projections/\"\n",
    "# file_name = \"distributions_dict\"\n",
    "# with open(\"{}{}.json\".format(data_location,file_name), 'r') as fp:\n",
    "#     SL_proj_dict = json.load(fp)\n",
    "    \n",
    "# def SLR_proj_extractor(SL_proj_dict,AIS_config,rcp,year):\n",
    "#     '''\n",
    "#     Function for getting SLR projections for a given Antarctic icesheet, rcp and year\n",
    "#     Years start as 2020 and go up in lots of 10 until 2150 (2100 for dp16)\n",
    "#     '''\n",
    "#     SLR_prob_dict = SL_proj_dict[\"('{}', '{}', {})\".format(AIS_config,rcp,year)]\n",
    "#     SLR_median_prob = np.max([float(x) for x in list(SLR_prob_dict.keys())])\n",
    "#     SLR_median_MSL = float(SLR_prob_dict[str(SLR_median_prob)])/1000 #units is m\n",
    "\n",
    "#     return(SLR_median_MSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_figure(\n",
    "#     view,tide_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin,proj_time,time\n",
    "#     ):\n",
    "#     # Extract the right model from the dictionary of model\n",
    "#     lagoon_model_dict = model_dicts_through_time_dict[time]['lagoon']\n",
    "#     ocean_model_dict = model_dicts_through_time_dict[time]['ocean']\n",
    "    \n",
    "#     ########### get the MSL bin based on slider value\n",
    "#     msl_proj = SLR_proj_extractor(SL_proj_dict,'k14','26','{}'.format(proj_time))\n",
    "# #     ocean_model_dict['variables']['MSL']['bin_edges'][0]\n",
    "#     bin_count = bin_locator(msl_proj,ocean_model_dict['variables']['MSL']['bin_edges'][0])\n",
    "#     msl_bin = ocean_model_dict['variables']['MSL']['discretisation']['bin_names'][bin_count]\n",
    "    \n",
    "#     if view == 'Map':\n",
    "#         map_osm = folium.Map(location=[1.448888, 172.991794],zoom_start=11)\n",
    "#     elif view == 'Satellite':\n",
    "#         token = \"pk.eyJ1Ijoic2hhbm5vbi1iZW5ndHNvbiIsImEiOiJja3F1Y2Q0dHEwMzYwMm9wYmtzYzk2bDZuIn0.5jGMyEiJdmXs1HL7x3ThPw\" # your mapbox token\n",
    "#         tileurl = 'https://api.mapbox.com/v4/mapbox.satellite/{z}/{x}/{y}@2x.png?access_token=' + str(token)\n",
    "\n",
    "#         map_osm = folium.Map(location=[1.448888, 172.991794], zoom_start=11, tiles=tileurl, attr='Mapbox')\n",
    "        \n",
    "#     twl_bin_edges = [round(x,2) for x in ocean_model_dict['variables']['TWL']['bin_edges'][0]]\n",
    "#     twl_bins = ocean_model_dict['variables']['TWL']['discretisation']['bin_names']\n",
    "\n",
    "#     colours_rgb = matplotlib.pyplot.get_cmap('seismic')(np.arange(0,1+1/len(twl_bins),1/(len(twl_bins)-1)))\n",
    "#     colour_hex_dict = {i:rgb2hex(int(255*colours_rgb[i][0]),int(255*colours_rgb[i][1]),int(255*(colours_rgb[i][2]))) for i in np.arange(0,len(twl_bins),1)}\n",
    "    \n",
    "#     ################################################\n",
    "    \n",
    "#     # Create an empty dictionary for the evidence and populate as you go\n",
    "#     ocean_evidence_dict = {}\n",
    "    \n",
    "#     for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin],\n",
    "#                                 ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore']):\n",
    "\n",
    "#         ## Set in the evidence dict to be as indicated in the dropdown\n",
    "#         bin_index = ocean_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "#         # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "#         evidence = [0 for x in ocean_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "#         evidence[bin_index] = 1\n",
    "#         ocean_evidence_dict.update({\n",
    "#             var_name:evidence\n",
    "#         })\n",
    "    \n",
    "#     # Create a list of variables that are location specific to set as evidence in the network\n",
    "#     variable_list = ['reef_width','reef_depth','forereef_slope','shore_dir']\n",
    "    \n",
    "#     # get the probability dictionary\n",
    "#     location_probabilities_dict = location_probabilities(ocean_evidence_dict,ocean_model_dict,variable_list,df_ocean_profiles)\n",
    "    \n",
    "#     # Create dataframe to plot\n",
    "#     df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "#     df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "#     data_ocean = data2geojson(df_twl_locations)\n",
    "    \n",
    "#     colors_hex_points_ocean = [colour_hex_dict[x] for x in df_twl_locations.most_likely_twl]\n",
    "    \n",
    "#     #####################################################################\n",
    "    \n",
    "#     # Create an empty dictionary for the evidence and populate as you go\n",
    "#     lagoon_evidence_dict = {}\n",
    "    \n",
    "#     for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin],\n",
    "#                                 ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore','wind_u','wind_v']):\n",
    "\n",
    "#         ## Set in the evidence dict to be as indicated in the dropdown\n",
    "#         bin_index = lagoon_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "#         # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "#         evidence = [0 for x in lagoon_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "#         evidence[bin_index] = 1\n",
    "#         lagoon_evidence_dict.update({\n",
    "#             var_name:evidence\n",
    "#         })\n",
    "    \n",
    "#     # Create a list of variables that are location specific to set as evidence in the network\n",
    "#     variable_list = []\n",
    "    \n",
    "#     # get the probability dictionary\n",
    "#     location_probabilities_dict = location_probabilities(lagoon_evidence_dict,lagoon_model_dict,variable_list,df_lagoon_profiles)\n",
    "    \n",
    "#     # Create dataframe to plot\n",
    "#     df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "#     df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "#     data_lagoon = data2geojson(df_twl_locations)\n",
    "    \n",
    "#     colors_hex_points_lagoon = [colour_hex_dict[x] for x in df_twl_locations.most_likely_twl]\n",
    "    \n",
    "#     #####################################################################\n",
    "    \n",
    "#     features_list = data_ocean['features']+data_lagoon['features']\n",
    "    \n",
    "#     data = data_ocean\n",
    "#     data.update({\n",
    "#         'features':features_list\n",
    "#     })\n",
    "    \n",
    "#     colors_hex_points = colors_hex_points_ocean+colors_hex_points_lagoon\n",
    "    \n",
    "#     #####################################################################\n",
    "\n",
    "#     for feature,color in zip(features_list,colors_hex_points):\n",
    "#         feature['properties'] = {'color':color,'weight':1,'markerColor':color,'fillOpacity':1,'fillColor':color}\n",
    "#         long,lat = feature['geometry']['coordinates']\n",
    "        \n",
    "#         marker = folium.CircleMarker([lat,long],color=color,\n",
    "#                                     # popup='<img src={}_{}.png>'.format(int(long*1000),int(lat*1000)),\n",
    "#                                    fill_color=color,fill=True,fill_opacity='1',radius=5)\n",
    "#         marker.add_to(map_osm)\n",
    "        \n",
    "#     twl_bin_edge_labels = ['{} to {} m'.format(\n",
    "#         x,y) for x,y in zip(twl_bin_edges[:-1],twl_bin_edges[1:])]\n",
    "        \n",
    "#     output_list = []\n",
    "#     for rgb_color in colours_rgb:\n",
    "#         output = plt.scatter([],[],color=rgb_color)\n",
    "#         output_list.append(output)\n",
    "        \n",
    "#     legend = plt.legend(output_list,twl_bin_edge_labels,title='Total water level anomaly',fontsize=10)\n",
    "#     plt.setp(legend.get_title(),fontsize=12)\n",
    "    \n",
    "#     plt.axis('off')\n",
    "#     plt.savefig('legend.png')\n",
    "    \n",
    "#     plt.close()\n",
    "    \n",
    "#     url = (\n",
    "#         \"legend.png\"\n",
    "#     )    \n",
    "    \n",
    "#     FloatImage(url, bottom=55, left=55).add_to(map_osm)\n",
    "    \n",
    "#     map_osm.save('test.html')\n",
    "        \n",
    "#     return(map_osm)\n",
    "\n",
    "# # compile the figure\n",
    "# lagoon_model_dict,ocean_model_dict = initialise_model_dictionaries()\n",
    "# tide_bins = ocean_model_dict['variables']['Tide']['discretisation']['bin_names']\n",
    "# wave_height_bins = ocean_model_dict['variables']['Hs_offshore']['discretisation']['bin_names']\n",
    "# wave_period_bin = ocean_model_dict['variables']['Tm_offshore']['discretisation']['bin_names']\n",
    "# wave_direction_bin = ocean_model_dict['variables']['Dir_offshore']['discretisation']['bin_names']\n",
    "# wind_u_bin = lagoon_model_dict['variables']['wind_u']['discretisation']['bin_names']\n",
    "# wind_v_bin = lagoon_model_dict['variables']['wind_u']['discretisation']['bin_names']\n",
    "# time = list(model_dicts_through_time_dict.keys())  \n",
    "\n",
    "# # Create the plot with the widget\n",
    "# map_osm = interact_manual(test_figure,\n",
    "#                 view = widgets.Dropdown(options=['Map','Satellite'],value='Map',description='View type',disabled=False),\n",
    "#                 tide_bin = widgets.Dropdown(options=tide_bins,value='Mid',description='Tide',disabled=False),\n",
    "#                 wave_height_bin = widgets.Dropdown(options=wave_height_bins,value='Mid',description='Wave height',disabled=False),\n",
    "#                 wave_period_bin = widgets.Dropdown(options=wave_period_bin,value='Mid',description='Wave period',disabled=False),\n",
    "#                 wave_direction_bin = widgets.Dropdown(options=wave_direction_bin,value='NNE',description='Wave direction',disabled=False),\n",
    "#                 wind_u_bin = widgets.Dropdown(options=wind_u_bin,value='Mid',description='Wind u',disabled=False),\n",
    "#                 wind_v_bin = widgets.Dropdown(options=wind_v_bin,value='Mid',description='Wind v',disabled=False),\n",
    "#                 proj_time = widgets.IntSlider(min=2020,max=2150,step=10,value=2020,description='SLR prediction'),\n",
    "#                 time = widgets.Dropdown(options=time,value=727930.0,description='Time',disabled=False)\n",
    "#                )\n",
    "\n",
    "# map_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "2344856/V5HIVSEQ": {
     "DOI": "10.1017/S0263593300020782",
     "URL": "https://www.cambridge.org/core/journals/earth-and-environmental-science-transactions-of-royal-society-of-edinburgh/article/an-alternative-astronomical-calibration-of-the-lower-pleistocene-timescale-based-on-odp-site-677/D02E93BFBF418256AD00642C8A98277C",
     "abstract": "Ocean Drilling Program (ODP) Site 677 provided excellent material for high resolution stable isotope analysis of both benthonic and planktonic foraminifera through the entire Pleistocene and upper Pliocene. The oxygen isotope record is readily correlated with the SPECMAP stack (Imbrie et al. 1984) and with the record from DSDP 607 (Ruddiman et al. 1986) but a significantly better match with orbital models is obtained by departing from the timescale proposed by these authors below Stage 16 (620 000 years). It is the stronger contribution from the precession signal in the record from ODP Site 677 that provides the basis for the revised timescale. Our proposed modification to the timescale would imply that the currently adopted radiometric dates for the Matuyama–Brunhes boundary, the Jaramillo and Olduvai Subchrons and the Gauss–Matuyama boundary underestimate their true astronomical ages by between 5 and 7%.",
     "accessed": {
      "day": 19,
      "month": 5,
      "year": 2020
     },
     "author": [
      {
       "family": "Shackleton",
       "given": "N. J."
      },
      {
       "family": "Berger",
       "given": "A."
      },
      {
       "family": "Peltier",
       "given": "W. R."
      }
     ],
     "container-title": "Earth and Environmental Science Transactions of The Royal Society of Edinburgh",
     "id": "2344856/V5HIVSEQ",
     "issue": "4",
     "issued": {
      "year": 1990
     },
     "language": "en",
     "note": "citation key: shackleton1990alternative",
     "page": "251-261",
     "page-first": "251",
     "title": "An alternative astronomical calibration of the lower Pleistocene timescale based on ODP Site 677",
     "type": "article-journal",
     "volume": "81"
    }
   }
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "744px",
    "left": "1596px",
    "right": "20px",
    "top": "131px",
    "width": "279px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
