{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the most up-to-date version of the pilot model for total water level on Tarawa, currently for only two locations (one lagoon side and one ocean side).\n",
    "\n",
    "Currently needs work:\n",
    "- Incorporating MEI into network\n",
    "- Adjusting the binning of the MSL distributions to account for future SLR\n",
    "- Adding SLR projections as evidence option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:06.245034Z",
     "start_time": "2021-06-10T02:18:06.200060Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:09.215577Z",
     "start_time": "2021-06-10T02:18:06.246033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-leaflet/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, time, timedelta\n",
    "import pysmile\n",
    "import pysmile_license\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('/src/python_classes')\n",
    "import rpy2\n",
    "# os.environ['R_HOME'] = 'C:\\ProgramData\\Anaconda3\\Lib\\R'\n",
    "# %load_ext rpy2.ipython\n",
    "!jupyter nbextension enable --py --sys-prefix ipyleaflet\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipyleaflet import *\n",
    "import ipywidgets as widgets\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import geojson\n",
    "import folium\n",
    "from folium.plugins import FloatImage as FloatImage\n",
    "from colormap import rgb2hex\n",
    "import rpy2\n",
    "os.environ['R_HOME'] = '/lib/R'\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "from BNModel import BNModel\n",
    "\n",
    "from preprocessing_all_points import *\n",
    "from preprocessing_points_spatially_temporally import *\n",
    "from compile_model_t import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:09.301509Z",
     "start_time": "2021-06-10T02:18:09.217559Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### set location of file storage\n",
    "# folder = 'BN_antonio_data'\n",
    "# try:\n",
    "#     os.makedirs(folder)\n",
    "# except FileExistsError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:16.558430Z",
     "start_time": "2021-06-10T02:18:09.303508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import and preprocess data\n",
    "df_lagoon_profiles,df_ocean_profiles,inundation_dict,winds_dict,waves_dict,tide_dict,sla_dict,time_dict = \\\n",
    "    loading_tarawa_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Variable Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:00.324519Z",
     "start_time": "2021-06-10T02:19:00.237568Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def initialise_model_dictionaries():\n",
    "    #### Don't include spaces in bin names. if no discretisation, just leave out that key\n",
    "    lagoon_model_dict = {\n",
    "        'variables':{\n",
    "            'wind_u':{\n",
    "                'label':'Wind u vector (m/s)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'wind_v':{\n",
    "                'label':r'Wind v vector (m/s)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'uniform',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'Hs_offshore':{\n",
    "                'label':'Offshore wave height (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','HighMid','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'Tm_offshore':{\n",
    "                'label':'Offshore wave period (s)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'Dir_offshore':{\n",
    "                'label':r'Offshore wave direction (degrees)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':8,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['NNE','ENE','ESE','SSE','SSW','WSW','WNW','NNW']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'TWL':{\n",
    "                'label':'Total water level (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':7,\n",
    "                    'strategy':'binned',\n",
    "                    'bin_names':['VeryLow','Low','LowMid','Mid','MidHigh','High','VeryHigh'],\n",
    "                    'bin_edges':np.arange(-1,3.0,0.5)\n",
    "                },\n",
    "                'child_nodes':[]\n",
    "            },\n",
    "            'TWL_less_Tide':{\n",
    "                'label':'Total water level less tide (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL']\n",
    "            },\n",
    "            'MSL':{\n",
    "                'label':'Mean sea level (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'Tide':{\n",
    "                'label':'Tide (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL']\n",
    "            }\n",
    "        },\n",
    "        'training_frac':0.8,\n",
    "        'bootstrap_reps':1\n",
    "    }\n",
    "    ocean_model_dict = {\n",
    "       'variables':{\n",
    "           'Tm_offshore':{\n",
    "                'label':'Wave period offshore (?)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'Hs_offshore':{\n",
    "                'label':'Wave height offshore (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'Dir_offshore':{\n",
    "                'label':'Wave direction offshore (degrees)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':8,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['NNE','ENE','ESE','SSE','SSW','WSW','WNW','NNW']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'MSL':{\n",
    "                'label':'Mean sea level (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'Tide':{\n",
    "                'label':'Tide (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL']\n",
    "            },\n",
    "            'TWL':{\n",
    "                'label':'Total water level (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':7,\n",
    "                    'strategy':'binned',\n",
    "                    'bin_names':['VeryLow','Low','LowMid','Mid','MidHigh','High','VeryHigh'],\n",
    "                    'bin_edges':np.arange(-1,3.0,0.5)\n",
    "                },\n",
    "                'child_nodes':[]\n",
    "            },\n",
    "            'TWL_less_Tide':{\n",
    "                'label':'Total water level less tide (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL']\n",
    "            },\n",
    "            'reef_width':{\n",
    "                'label':'Reef width (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'reef_depth':{\n",
    "                'label':'Reef depth (m)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'forereef_slope':{\n",
    "                'label':'Fore reef slope (degrees)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            },\n",
    "            'shore_dir':{\n",
    "                'label':'Shoreline direction (degrees)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':3,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['NE','S','NW']\n",
    "                },\n",
    "                'child_nodes':['TWL_less_Tide']\n",
    "            }\n",
    "       },\n",
    "        'training_frac':0.8,\n",
    "        'bootstrap_reps':1\n",
    "    }\n",
    "    return(lagoon_model_dict,ocean_model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_figure(\n",
    "#     view,tide_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin,proj_time,time_min,time_max\n",
    "#     ):\n",
    "    \n",
    "#     for lower_idx,upper_idx in zip(np.arange(time_min,len(inundation_dict['Time']),100),np.arange(100+time_min,len(inundation_dict['Time']),100)):\n",
    "#         print(lower_idx,upper_idx)\n",
    "\n",
    "    \n",
    "#     # Extract the right model from the dictionary of model\n",
    "#     lagoon_model_dict = model_dicts_through_time_dict[time_max]['lagoon']\n",
    "#     ocean_model_dict = model_dicts_through_time_dict[time_min]['ocean']\n",
    "    \n",
    "#     ########### get the MSL bin based on slider value\n",
    "#     msl_proj = SLR_proj_extractor(SL_proj_dict,'k14','26','{}'.format(proj_time))\n",
    "#     bin_count = bin_locator(msl_proj,ocean_model_dict['variables']['MSL']['bin_edges'][0])\n",
    "#     msl_bin = ocean_model_dict['variables']['MSL']['discretisation']['bin_names'][bin_count]\n",
    "   \n",
    "#     ################################################\n",
    "    \n",
    "#     # Create an empty dictionary for the evidence and populate as you go\n",
    "#     ocean_evidence_dict = {}\n",
    "    \n",
    "#     for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin],\n",
    "#                                 ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore']):\n",
    "\n",
    "#         ## Set in the evidence dict to be as indicated in the dropdown\n",
    "#         bin_index = ocean_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "#         # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "#         evidence = [0 for x in ocean_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "#         evidence[bin_index] = 1\n",
    "#         ocean_evidence_dict.update({\n",
    "#             var_name:evidence\n",
    "#         })\n",
    "    \n",
    "#     # Create a list of variables that are location specific to set as evidence in the network\n",
    "#     variable_list = ['reef_width','reef_depth','forereef_slope','shore_dir']\n",
    "    \n",
    "#     # get the probability dictionary\n",
    "#     location_probabilities_dict = location_probabilities(ocean_evidence_dict,ocean_model_dict,variable_list,df_ocean_profiles)\n",
    "    \n",
    "#     # Create dataframe to plot\n",
    "#     df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "#     df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "#     #####################################################################\n",
    "    \n",
    "#     # Create an empty dictionary for the evidence and populate as you go\n",
    "#     lagoon_evidence_dict = {}\n",
    "    \n",
    "#     for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin],\n",
    "#                                 ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore','wind_u','wind_v']):\n",
    "\n",
    "#         ## Set in the evidence dict to be as indicated in the dropdown\n",
    "#         bin_index = lagoon_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "#         # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "#         evidence = [0 for x in lagoon_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "#         evidence[bin_index] = 1\n",
    "#         lagoon_evidence_dict.update({\n",
    "#             var_name:evidence\n",
    "#         })\n",
    "    \n",
    "#     # Create a list of variables that are location specific to set as evidence in the network\n",
    "#     variable_list = []\n",
    "    \n",
    "#     # get the probability dictionary\n",
    "#     location_probabilities_dict = location_probabilities(lagoon_evidence_dict,lagoon_model_dict,variable_list,df_lagoon_profiles)\n",
    "    \n",
    "#     # Create dataframe to plot\n",
    "#     df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "#     df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#     return()\n",
    "\n",
    "# # compile the figure\n",
    "# lagoon_model_dict,ocean_model_dict = initialise_model_dictionaries()\n",
    "# tide_bins = ocean_model_dict['variables']['Tide']['discretisation']['bin_names']\n",
    "# wave_height_bins = ocean_model_dict['variables']['Hs_offshore']['discretisation']['bin_names']\n",
    "# wave_period_bin = ocean_model_dict['variables']['Tm_offshore']['discretisation']['bin_names']\n",
    "# wave_direction_bin = ocean_model_dict['variables']['Dir_offshore']['discretisation']['bin_names']\n",
    "# wind_u_bin = lagoon_model_dict['variables']['wind_u']['discretisation']['bin_names']\n",
    "# wind_v_bin = lagoon_model_dict['variables']['wind_u']['discretisation']['bin_names']\n",
    "# time = list(df_lagoon.time) \n",
    "\n",
    "# # Create the plot with the widget\n",
    "# interact_manual(test_figure,\n",
    "#                 view = widgets.Dropdown(options=['Map','Satellite'],value='Map',description='View type',disabled=False),\n",
    "#                 tide_bin = widgets.Dropdown(options=tide_bins,value='Mid',description='Tide',disabled=False),\n",
    "#                 wave_height_bin = widgets.Dropdown(options=wave_height_bins,value='Mid',description='Wave height',disabled=False),\n",
    "#                 wave_period_bin = widgets.Dropdown(options=wave_period_bin,value='Mid',description='Wave period',disabled=False),\n",
    "#                 wave_direction_bin = widgets.Dropdown(options=wave_direction_bin,value='NNE',description='Wave direction',disabled=False),\n",
    "#                 wind_u_bin = widgets.Dropdown(options=wind_u_bin,value='Mid',description='Wind u',disabled=False),\n",
    "#                 wind_v_bin = widgets.Dropdown(options=wind_v_bin,value='Mid',description='Wind v',disabled=False),\n",
    "#                 proj_time = widgets.IntSlider(min=2020,max=2150,step=10,value=2020,description='SLR prediction'),\n",
    "#                 time_min = widgets.Dropdown(options=time,value=time[0],description='Time',disabled=False),\n",
    "#                 time_max = widgets.Dropdown(options=time,value=time[0],description='Time',disabled=False)\n",
    "#                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One network per time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ocean_probabilities_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0572c9b75fd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpredicted_lag_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'TWL_t_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_lag_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mdf_ocean_profiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mocean_probabilities_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         ocean_model_dict['variables'].update({\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ocean_probabilities_dict' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model_dicts_through_time_dict = {}\n",
    "\n",
    "time_min = inundation_dict['Time'][0]\n",
    "time_step = timedelta(hours=24)\n",
    "max_steps = 7\n",
    "\n",
    "times_for_model = []\n",
    "\n",
    "for step in np.arange(0,max_steps,1):\n",
    "    time_at_step = time_min+time_step*step\n",
    "    \n",
    "    times_for_model.append(time_at_step)\n",
    "    \n",
    "times_for_model = times_for_model[:10]\n",
    "    \n",
    "for time_min,time_max in zip(times_for_model[:-1],times_for_model[1:]):\n",
    "    lower_idx = np.abs([x-time_min for x in inundation_dict['Time']]).argmin()\n",
    "    upper_idx = np.abs([x-time_max for x in inundation_dict['Time']]).argmin()\n",
    "       \n",
    "    inundation_time_slice_dict = {var:inundation_dict[var][lower_idx:upper_idx,:] for var in ['TWL','Tide']}\n",
    "    inundation_time_slice_dict['Time'] = inundation_dict['Time'][lower_idx:upper_idx]\n",
    "    inundation_time_slice_dict['Ptos'] = inundation_dict['Ptos']\n",
    "    winds_time_slice_dict = {\n",
    "        'wind_u':winds_dict['wind_u'][lower_idx:upper_idx],'wind_v':winds_dict['wind_v'][lower_idx:upper_idx]}\n",
    "    waves_time_slice_dict = {var:waves_dict[var][lower_idx:upper_idx,:] for var in ['Diro','Hso','Tmo','Tpo']}\n",
    "    waves_time_slice_dict['Timeo'] = waves_dict['Timeo'][lower_idx:upper_idx]\n",
    "    \n",
    "    tide_time_slice_dict = {'Tide':tide_dict['Tide'][lower_idx:upper_idx]}\n",
    "    sla_time_slice_dict = {'MSL':sla_dict['MSL'][lower_idx:upper_idx]}\n",
    "    time_slice_dict = {'time':time_dict['time'][lower_idx:upper_idx]}\n",
    "    \n",
    "    if time_min==times_for_model[0]:\n",
    "        lagoon_model_dict,ocean_model_dict = initialise_model_dictionaries()\n",
    "        predicted_lag_variables = []\n",
    "    else:\n",
    "        predicted_lag_variables = ['TWL_t_1']\n",
    "        for var in predicted_lag_variables:\n",
    "            df_ocean_profiles[var] = [item for key,item in ocean_probabilities_dict.items()]\n",
    "            \n",
    "        ocean_model_dict['variables'].update({\n",
    "                'TWL_t_1':{\n",
    "                'label':'Total water level bin at t-1 (1-5?)',\n",
    "                'discretisation':{\n",
    "                    'n_bins':5,\n",
    "                    'strategy':'kmeans',\n",
    "                    'bin_names':['1','2','3','4','5']\n",
    "                },\n",
    "                'child_nodes':['TWL']\n",
    "            }\n",
    "        })\n",
    "\n",
    "    df_ocean,df_lagoon,ocean_data_dict,lagoon_data_dict = \\\n",
    "        preprocessing_points_spatially_temporally(df_lagoon_profiles,\n",
    "                                                  df_ocean_profiles,\n",
    "                                                  inundation_time_slice_dict,\n",
    "                                                  winds_time_slice_dict,\n",
    "                                                  waves_time_slice_dict,\n",
    "                                                  tide_time_slice_dict,\n",
    "                                                  sla_time_slice_dict,\n",
    "                                                  time_slice_dict,\n",
    "                                                  predicted_lag_variables)\n",
    "    \n",
    "    lagoon_model_dict,ocean_model_dict = create_BN_time_t(lagoon_model_dict,\n",
    "                                                          lagoon_data_dict,\n",
    "                                                          df_lagoon,\n",
    "                                                          ocean_model_dict,\n",
    "                                                          df_ocean,\n",
    "                                                          ocean_data_dict)\n",
    "    # Create an empty evidence dict\n",
    "    evidence_dict = {}\n",
    "    \n",
    "    # Add evidence to model dict\n",
    "    ocean_model_dict = BNModel().add_evidence_to_dict(ocean_model_dict,evidence_dict)\n",
    "    lagoon_model_dict = BNModel().add_evidence_to_dict(lagoon_model_dict,evidence_dict)\n",
    "\n",
    "    # Set evidence and get beliefs\n",
    "    ocean_model_dict = BNModel().update_evidence(ocean_model_dict)\n",
    "    lagoon_model_dict = BNModel().update_evidence(lagoon_model_dict)\n",
    "    \n",
    "    # Get the resulting TWL probabilities (right now for a generic location)\n",
    "    resulting_probs_ocean = ocean_model_dict['variables']['TWL']['resulting_probs']\n",
    "    resulting_probs_lagoon = lagoon_model_dict['variables']['TWL']['resulting_probs']\n",
    "    \n",
    "    model_dicts_through_time_dict.update({\n",
    "        time_dict['time'][lower_idx]:{\n",
    "            'lagoon':lagoon_model_dict,\n",
    "            'ocean':ocean_model_dict\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Timestamp('1993-01-04 00:00:00') is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f80ec34d622a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlag_timestep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_twl_ocean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_ocean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'TWL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_twl_ocean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_twl_ocean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlag_timestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Timestamp('1993-01-04 00:00:00') is not in list"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    }
   ],
   "source": [
    "i =0 \n",
    "for (lat,long),group in df_ocean.groupby(['lat','long']):\n",
    "    i+=1\n",
    "print(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'DatetimeIndex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-66445a10fc00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlag_timestep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'TWL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_group_lagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlag_timestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdf_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TWL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'DatetimeIndex'"
     ]
    }
   ],
   "source": [
    "lag_timestep = timedelta(3)\n",
    "df_group = group[['time','TWL']].set_index('time',inplace=False)\n",
    "df_group_lagged = {(df_group.index-lag_timestep):df_group['TWL']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining times of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to have a look at how the resulting probabilties varied through time\n",
    "resulting_probs_dict = {}\n",
    "\n",
    "for time,model_dict['lagoon'] in model_dicts_through_time_dict.items():\n",
    "    \n",
    "    #Create an empty dictionary of evidence for now\n",
    "    evidence_dict = {}\n",
    "    \n",
    "    # Add evidence to model dict\n",
    "    model_location_dict = BNModel().add_evidence_to_dict(model_dict,evidence_dict)\n",
    "\n",
    "    # Set evidence and get beliefs\n",
    "    model_location_dict = BNModel().update_evidence(model_location_dict)\n",
    "    \n",
    "    # Get the resulting TWL probabilities (right now for a generic location)\n",
    "    resulting_probs = model_location_dict['variables']['TWL']['resulting_probs']\n",
    "    \n",
    "    # Add the resulting probabilities to a dictionary\n",
    "    resulting_probs_dict.update({\n",
    "        time:resulting_probs[0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_probs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the reef and shoreline profile information\n",
    "df_ocean_profiles = pd.read_csv('/src/Dataset/D8_tarawa_inundation/Profiles_definition_outer_reef_xyxy_processed.txt')\n",
    "df_lagoon_profiles = pd.read_csv('/src/Dataset/D8_tarawa_inundation/Profiles_definition_inner_lagoon_xyxy.txt',delim_whitespace=True,header=None)\n",
    "df_lagoon_profiles.columns = ['reef_long','reef_lat','shore_long','shore_lat','reef_depth']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_locator(value,bin_edges):\n",
    "    '''\n",
    "    function used for determining the index of the appropriate bin for a numerical value.\n",
    "    '''\n",
    "    i=0\n",
    "    for edge_1,edge_2 in zip(bin_edges[:-1],bin_edges[1:]):\n",
    "        if (value>edge_1)&(value<=edge_2):\n",
    "            loc_bin = i\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    if value<=bin_edges[0]:\n",
    "        loc_bin = 0\n",
    "\n",
    "    if value>=bin_edges[-1]:\n",
    "        loc_bin = len(bin_edges)-2\n",
    "\n",
    "    return(loc_bin)\n",
    "\n",
    "def model_location(model_dict,location_details,evidence_dict,variable_list):\n",
    "    \n",
    "    '''\n",
    "    function for adding the location information for one side model to the evidence dictionary\n",
    "    '''\n",
    "    \n",
    "    for variable in variable_list:\n",
    "\n",
    "        bin_edges = model_dict['variables'][variable]['bin_edges'][0]\n",
    "        value = location_details[variable]\n",
    "\n",
    "        var_bin = bin_locator(value,bin_edges)\n",
    "        \n",
    "        evidence_array = [0]*(len(bin_edges)-1)\n",
    "        evidence_array[var_bin] = 1\n",
    "        \n",
    "        evidence_dict.update({\n",
    "            variable:evidence_array\n",
    "        })\n",
    "\n",
    "    # Add evidence to model dict\n",
    "    model_location_dict = BNModel().add_evidence_to_dict(model_dict,evidence_dict)\n",
    "    \n",
    "    # Set evidence and get beliefs\n",
    "    model_location_dict = BNModel().update_evidence(model_location_dict)\n",
    "    \n",
    "    return(model_location_dict)\n",
    "\n",
    "def location_probabilities(evidence_dict,model_dict,variable_list,df_profiles):\n",
    "    '''\n",
    "    \n",
    "    Function for setting evidence and determing probabilties for twl at each point around the island based \n",
    "    on the reef characteristics at each location\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    location_probabilities_dict = {}\n",
    "#     figure_dict = {}\n",
    "    \n",
    "    for index,row in df_profiles.iterrows():\n",
    "\n",
    "        model_location_dict = model_location(model_dict,row,evidence_dict,variable_list)\n",
    "        location_probabilities = model_location_dict['variables']['TWL']['resulting_probs'][0]\n",
    "\n",
    "        df_location_probabilities = pd.DataFrame.from_dict(location_probabilities,orient='index')\n",
    "        \n",
    "#         # Create figure for popup\n",
    "#         fig = plt.figure(figsize=(2,2))\n",
    "#         plt.bar(x=df_location_probabilities.index,height=df_location_probabilities[0])\n",
    "#         plt.savefig('{}_{}.png'.format(int(row.reef_long*1000),int(row.reef_lat*1000)))\n",
    "#         plt.close()\n",
    "        \n",
    "        largest_cat = df_location_probabilities.idxmax()[0]\n",
    "\n",
    "        location_probabilities_dict.update({\n",
    "            (row.reef_long,row.reef_lat):\\\n",
    "                model_dict['variables']['TWL']['discretisation']['bin_names'].index(largest_cat)\n",
    "        })\n",
    "        \n",
    "#         figure_dict.update({\n",
    "#             (row.reef_long,row.reef_lat):fig#html_graph\n",
    "#         })\n",
    "        \n",
    "    return(location_probabilities_dict)\n",
    "\n",
    "def data2geojson(df):\n",
    "    features = []\n",
    "    insert_features = lambda X: features.append(\n",
    "            geojson.Feature(geometry=geojson.Point((X[\"long\"],\n",
    "                                                    X[\"lat\"])),\n",
    "                            properties=dict(name=X[\"most_likely_twl\"])))\n",
    "    df.apply(insert_features, axis=1)\n",
    "        \n",
    "    return(geojson.FeatureCollection(features))\n",
    "\n",
    "# Load SLR Projections\n",
    "data_location = \"/src/Dataset/D7_MSL_projections/\"\n",
    "file_name = \"distributions_dict\"\n",
    "with open(\"{}{}.json\".format(data_location,file_name), 'r') as fp:\n",
    "    SL_proj_dict = json.load(fp)\n",
    "    \n",
    "def SLR_proj_extractor(SL_proj_dict,AIS_config,rcp,year):\n",
    "    '''\n",
    "    Function for getting SLR projections for a given Antarctic icesheet, rcp and year\n",
    "    Years start as 2020 and go up in lots of 10 until 2150 (2100 for dp16)\n",
    "    '''\n",
    "    SLR_prob_dict = SL_proj_dict[\"('{}', '{}', {})\".format(AIS_config,rcp,year)]\n",
    "    SLR_median_prob = np.max([float(x) for x in list(SLR_prob_dict.keys())])\n",
    "    SLR_median_MSL = float(SLR_prob_dict[str(SLR_median_prob)])/1000 #units is m\n",
    "\n",
    "    return(SLR_median_MSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51e70ab518144ff966db9088228a9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='View type', options=('Map', 'Satellite'), value='Map'), Dropdown(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.test_figure(view, tide_bin, wave_height_bin, wave_period_bin, wave_direction_bin, wind_u_bin, wind_v_bin, proj_time, time)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_figure(\n",
    "    view,tide_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin,proj_time,time\n",
    "    ):\n",
    "    # Extract the right model from the dictionary of model\n",
    "    lagoon_model_dict = model_dicts_through_time_dict[time]['lagoon']\n",
    "    ocean_model_dict = model_dicts_through_time_dict[time]['ocean']\n",
    "    \n",
    "    ########### get the MSL bin based on slider value\n",
    "    msl_proj = SLR_proj_extractor(SL_proj_dict,'k14','26','{}'.format(proj_time))\n",
    "#     ocean_model_dict['variables']['MSL']['bin_edges'][0]\n",
    "    bin_count = bin_locator(msl_proj,ocean_model_dict['variables']['MSL']['bin_edges'][0])\n",
    "    msl_bin = ocean_model_dict['variables']['MSL']['discretisation']['bin_names'][bin_count]\n",
    "    \n",
    "    if view == 'Map':\n",
    "        map_osm = folium.Map(location=[1.448888, 172.991794],zoom_start=11)\n",
    "    elif view == 'Satellite':\n",
    "        token = \"pk.eyJ1Ijoic2hhbm5vbi1iZW5ndHNvbiIsImEiOiJja3F1Y2Q0dHEwMzYwMm9wYmtzYzk2bDZuIn0.5jGMyEiJdmXs1HL7x3ThPw\" # your mapbox token\n",
    "        tileurl = 'https://api.mapbox.com/v4/mapbox.satellite/{z}/{x}/{y}@2x.png?access_token=' + str(token)\n",
    "\n",
    "        map_osm = folium.Map(location=[1.448888, 172.991794], zoom_start=11, tiles=tileurl, attr='Mapbox')\n",
    "        \n",
    "    twl_bin_edges = [round(x,2) for x in ocean_model_dict['variables']['TWL']['bin_edges'][0]]\n",
    "    twl_bins = ocean_model_dict['variables']['TWL']['discretisation']['bin_names']\n",
    "\n",
    "    colours_rgb = matplotlib.pyplot.get_cmap('seismic')(np.arange(0,1+1/len(twl_bins),1/(len(twl_bins)-1)))\n",
    "    colour_hex_dict = {i:rgb2hex(int(255*colours_rgb[i][0]),int(255*colours_rgb[i][1]),int(255*(colours_rgb[i][2]))) for i in np.arange(0,len(twl_bins),1)}\n",
    "    \n",
    "    ################################################\n",
    "    \n",
    "    # Create an empty dictionary for the evidence and populate as you go\n",
    "    ocean_evidence_dict = {}\n",
    "    \n",
    "    for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin],\n",
    "                                ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore']):\n",
    "\n",
    "        ## Set in the evidence dict to be as indicated in the dropdown\n",
    "        bin_index = ocean_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "        # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "        evidence = [0 for x in ocean_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "        evidence[bin_index] = 1\n",
    "        ocean_evidence_dict.update({\n",
    "            var_name:evidence\n",
    "        })\n",
    "    \n",
    "    # Create a list of variables that are location specific to set as evidence in the network\n",
    "    variable_list = ['reef_width','reef_depth','forereef_slope','shore_dir']\n",
    "    \n",
    "    # get the probability dictionary\n",
    "    location_probabilities_dict = location_probabilities(ocean_evidence_dict,ocean_model_dict,variable_list,df_ocean_profiles)\n",
    "    \n",
    "    # Create dataframe to plot\n",
    "    df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "    df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    data_ocean = data2geojson(df_twl_locations)\n",
    "    \n",
    "    colors_hex_points_ocean = [colour_hex_dict[x] for x in df_twl_locations.most_likely_twl]\n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "    # Create an empty dictionary for the evidence and populate as you go\n",
    "    lagoon_evidence_dict = {}\n",
    "    \n",
    "    for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin],\n",
    "                                ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore','wind_u','wind_v']):\n",
    "\n",
    "        ## Set in the evidence dict to be as indicated in the dropdown\n",
    "        bin_index = lagoon_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "        # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "        evidence = [0 for x in lagoon_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "        evidence[bin_index] = 1\n",
    "        lagoon_evidence_dict.update({\n",
    "            var_name:evidence\n",
    "        })\n",
    "    \n",
    "    # Create a list of variables that are location specific to set as evidence in the network\n",
    "    variable_list = []\n",
    "    \n",
    "    # get the probability dictionary\n",
    "    location_probabilities_dict = location_probabilities(lagoon_evidence_dict,lagoon_model_dict,variable_list,df_lagoon_profiles)\n",
    "    \n",
    "    # Create dataframe to plot\n",
    "    df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "    df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    data_lagoon = data2geojson(df_twl_locations)\n",
    "    \n",
    "    colors_hex_points_lagoon = [colour_hex_dict[x] for x in df_twl_locations.most_likely_twl]\n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "    features_list = data_ocean['features']+data_lagoon['features']\n",
    "    \n",
    "    data = data_ocean\n",
    "    data.update({\n",
    "        'features':features_list\n",
    "    })\n",
    "    \n",
    "    colors_hex_points = colors_hex_points_ocean+colors_hex_points_lagoon\n",
    "    \n",
    "    #####################################################################\n",
    "\n",
    "    for feature,color in zip(features_list,colors_hex_points):\n",
    "        feature['properties'] = {'color':color,'weight':1,'markerColor':color,'fillOpacity':1,'fillColor':color}\n",
    "        long,lat = feature['geometry']['coordinates']\n",
    "        \n",
    "        marker = folium.CircleMarker([lat,long],color=color,\n",
    "                                    # popup='<img src={}_{}.png>'.format(int(long*1000),int(lat*1000)),\n",
    "                                   fill_color=color,fill=True,fill_opacity='1',radius=5)\n",
    "        marker.add_to(map_osm)\n",
    "        \n",
    "    twl_bin_edge_labels = ['{} to {} m'.format(\n",
    "        x,y) for x,y in zip(twl_bin_edges[:-1],twl_bin_edges[1:])]\n",
    "        \n",
    "    output_list = []\n",
    "    for rgb_color in colours_rgb:\n",
    "        output = plt.scatter([],[],color=rgb_color)\n",
    "        output_list.append(output)\n",
    "        \n",
    "    legend = plt.legend(output_list,twl_bin_edge_labels,title='Total water level anomaly',fontsize=10)\n",
    "    plt.setp(legend.get_title(),fontsize=12)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.savefig('legend.png')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    url = (\n",
    "        \"legend.png\"\n",
    "    )    \n",
    "    \n",
    "    FloatImage(url, bottom=55, left=55).add_to(map_osm)\n",
    "    \n",
    "    map_osm.save('test.html')\n",
    "        \n",
    "    return(map_osm)\n",
    "\n",
    "# compile the figure\n",
    "lagoon_model_dict,ocean_model_dict = initialise_model_dictionaries()\n",
    "tide_bins = ocean_model_dict['variables']['Tide']['discretisation']['bin_names']\n",
    "wave_height_bins = ocean_model_dict['variables']['Hs_offshore']['discretisation']['bin_names']\n",
    "wave_period_bin = ocean_model_dict['variables']['Tm_offshore']['discretisation']['bin_names']\n",
    "wave_direction_bin = ocean_model_dict['variables']['Dir_offshore']['discretisation']['bin_names']\n",
    "wind_u_bin = lagoon_model_dict['variables']['wind_u']['discretisation']['bin_names']\n",
    "wind_v_bin = lagoon_model_dict['variables']['wind_u']['discretisation']['bin_names']\n",
    "time = list(model_dicts_through_time_dict.keys())  \n",
    "\n",
    "# Create the plot with the widget\n",
    "map_osm = interact_manual(test_figure,\n",
    "                view = widgets.Dropdown(options=['Map','Satellite'],value='Map',description='View type',disabled=False),\n",
    "                tide_bin = widgets.Dropdown(options=tide_bins,value='Mid',description='Tide',disabled=False),\n",
    "                wave_height_bin = widgets.Dropdown(options=wave_height_bins,value='Mid',description='Wave height',disabled=False),\n",
    "                wave_period_bin = widgets.Dropdown(options=wave_period_bin,value='Mid',description='Wave period',disabled=False),\n",
    "                wave_direction_bin = widgets.Dropdown(options=wave_direction_bin,value='NNE',description='Wave direction',disabled=False),\n",
    "                wind_u_bin = widgets.Dropdown(options=wind_u_bin,value='Mid',description='Wind u',disabled=False),\n",
    "                wind_v_bin = widgets.Dropdown(options=wind_v_bin,value='Mid',description='Wind v',disabled=False),\n",
    "                proj_time = widgets.IntSlider(min=2020,max=2150,step=10,value=2020,description='SLR prediction'),\n",
    "                time = widgets.Dropdown(options=time,value=727930.0,description='Time',disabled=False)\n",
    "               )\n",
    "\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium.FloatImage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ce968f32d8c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFloatImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'folium.FloatImage'"
     ]
    }
   ],
   "source": [
    "import folium.FloatImage as FloatImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "2344856/V5HIVSEQ": {
     "DOI": "10.1017/S0263593300020782",
     "URL": "https://www.cambridge.org/core/journals/earth-and-environmental-science-transactions-of-royal-society-of-edinburgh/article/an-alternative-astronomical-calibration-of-the-lower-pleistocene-timescale-based-on-odp-site-677/D02E93BFBF418256AD00642C8A98277C",
     "abstract": "Ocean Drilling Program (ODP) Site 677 provided excellent material for high resolution stable isotope analysis of both benthonic and planktonic foraminifera through the entire Pleistocene and upper Pliocene. The oxygen isotope record is readily correlated with the SPECMAP stack (Imbrie et al. 1984) and with the record from DSDP 607 (Ruddiman et al. 1986) but a significantly better match with orbital models is obtained by departing from the timescale proposed by these authors below Stage 16 (620 000 years). It is the stronger contribution from the precession signal in the record from ODP Site 677 that provides the basis for the revised timescale. Our proposed modification to the timescale would imply that the currently adopted radiometric dates for the Matuyama–Brunhes boundary, the Jaramillo and Olduvai Subchrons and the Gauss–Matuyama boundary underestimate their true astronomical ages by between 5 and 7%.",
     "accessed": {
      "day": 19,
      "month": 5,
      "year": 2020
     },
     "author": [
      {
       "family": "Shackleton",
       "given": "N. J."
      },
      {
       "family": "Berger",
       "given": "A."
      },
      {
       "family": "Peltier",
       "given": "W. R."
      }
     ],
     "container-title": "Earth and Environmental Science Transactions of The Royal Society of Edinburgh",
     "id": "2344856/V5HIVSEQ",
     "issue": "4",
     "issued": {
      "year": 1990
     },
     "language": "en",
     "note": "citation key: shackleton1990alternative",
     "page": "251-261",
     "page-first": "251",
     "title": "An alternative astronomical calibration of the lower Pleistocene timescale based on ODP Site 677",
     "type": "article-journal",
     "volume": "81"
    }
   }
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "744px",
    "left": "1596px",
    "right": "20px",
    "top": "131px",
    "width": "279px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
