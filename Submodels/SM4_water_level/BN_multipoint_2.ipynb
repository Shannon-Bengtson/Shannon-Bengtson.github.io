{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the most up-to-date version of the pilot model for total water level on Tarawa, currently for only two locations (one lagoon side and one ocean side).\n",
    "\n",
    "Currently needs work:\n",
    "- Incorporating MEI into network\n",
    "- Adjusting the binning of the MSL distributions to account for future SLR\n",
    "- Adding SLR projections as evidence option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:06.245034Z",
     "start_time": "2021-06-10T02:18:06.200060Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:09.215577Z",
     "start_time": "2021-06-10T02:18:06.246033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-leaflet/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pysmile\n",
    "import pysmile_license\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('/src/python_classes')\n",
    "import rpy2\n",
    "# os.environ['R_HOME'] = 'C:\\ProgramData\\Anaconda3\\Lib\\R'\n",
    "# %load_ext rpy2.ipython\n",
    "!jupyter nbextension enable --py --sys-prefix ipyleaflet\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipyleaflet import *\n",
    "import ipywidgets as widgets\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import geojson\n",
    "import folium\n",
    "from colormap import rgb2hex\n",
    "import rpy2\n",
    "os.environ['R_HOME'] = '/lib/R'\n",
    "%load_ext rpy2.ipython\n",
    "from folium.plugins import FloatImage\n",
    "\n",
    "from BNModel import BNModel\n",
    "\n",
    "from preprocessing_all_points import *\n",
    "from preprocessing_points_spatially import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:09.301509Z",
     "start_time": "2021-06-10T02:18:09.217559Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### set location of file storage\n",
    "# folder = 'BN_antonio_data'\n",
    "# try:\n",
    "#     os.makedirs(folder)\n",
    "# except FileExistsError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:16.558430Z",
     "start_time": "2021-06-10T02:18:09.303508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import and preprocess data\n",
    "df_ocean,df_lagoon = preprocessing_points_spatially()\n",
    "df_lagoon,lagoon_data_dict = BN_Antonio_preprocessing_lagoon(df_lagoon)\n",
    "df_ocean,ocean_data_dict = BN_Antonio_preprocessing_ocean(df_ocean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Variable Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:00.234568Z",
     "start_time": "2021-06-10T02:19:00.159611Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Don't include spaces in bin names. if no discretisation, just leave out that key\n",
    "lagoon_model_dict = {\n",
    "    'variables':{\n",
    "        'wind_u':{\n",
    "            'label':'Wind u vector',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'wind_v':{\n",
    "            'label':r'Wind v vector',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'uniform',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'Hs_offshore':{\n",
    "            'label':'Offshore wave height',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','HighMid','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'Tm_offshore':{\n",
    "            'label':'Offshore wave period',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'Dir_offshore':{\n",
    "            'label':r'Offshore wave direction',\n",
    "            'discretisation':{\n",
    "                'n_bins':8,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['NNE','ENE','ESE','SSE','SSW','WSW','WNW','NNW']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'TWL':{\n",
    "            'label':'Total water level',\n",
    "            'discretisation':{\n",
    "                'n_bins':7,\n",
    "                'strategy':'binned',\n",
    "                'bin_names':['VeryLow','Low','LowMid','Mid','MidHigh','High','VeryHigh'],\n",
    "                'bin_edges':np.arange(-1,3.0,0.5)\n",
    "            },\n",
    "            'child_nodes':[]\n",
    "        },\n",
    "        'TWL_less_Tide':{\n",
    "            'label':'Total water level less tide',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL']\n",
    "        },\n",
    "        'MSL':{\n",
    "            'label':'Mean sea level',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'Tide':{\n",
    "            'label':'Tide',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL']\n",
    "        }\n",
    "    },\n",
    "    'training_frac':0.8,\n",
    "    'bootstrap_reps':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:00.324519Z",
     "start_time": "2021-06-10T02:19:00.237568Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ocean_model_dict = {\n",
    "   'variables':{\n",
    "       'Tm_offshore':{\n",
    "            'label':'Wave period offshore',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'Hs_offshore':{\n",
    "            'label':'Wave height offshore',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'Dir_offshore':{\n",
    "            'label':'Wave direction offshore',\n",
    "            'discretisation':{\n",
    "                'n_bins':8,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['NNE','ENE','ESE','SSE','SSW','WSW','WNW','NNW']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'MSL':{\n",
    "            'label':'Mean sea level',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'Tide':{\n",
    "            'label':'Tide',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL']\n",
    "        },\n",
    "        'TWL':{\n",
    "            'label':'Total water level',\n",
    "            'discretisation':{\n",
    "                'n_bins':7,\n",
    "                'strategy':'binned',\n",
    "                'bin_names':['VeryLow','Low','LowMid','Mid','MidHigh','High','VeryHigh'],\n",
    "                'bin_edges':np.arange(-1,3.0,0.5)\n",
    "            },\n",
    "            'child_nodes':[]\n",
    "        },\n",
    "        'TWL_less_Tide':{\n",
    "            'label':'Total water level less tide',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL']\n",
    "        },\n",
    "        'reef_width':{\n",
    "            'label':'Reef width',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'reef_depth':{\n",
    "            'label':'Reef depth',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'forereef_slope':{\n",
    "            'label':'Fore reef slope',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        },\n",
    "        'shore_dir':{\n",
    "            'label':'Shoreline direction',\n",
    "            'discretisation':{\n",
    "                'n_bins':3,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['NE','S','NW']\n",
    "            },\n",
    "            'child_nodes':['TWL_less_Tide']\n",
    "        } \n",
    "   },\n",
    "    'training_frac':0.8,\n",
    "    'bootstrap_reps':1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:00.754301Z",
     "start_time": "2021-06-10T02:19:00.326513Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bootstrap the data, and add it to the model_dict\n",
    "lagoon_model_dict = BNModel().bootstrap_data(lagoon_model_dict,lagoon_data_dict,df_lagoon)\n",
    "\n",
    "ocean_model_dict = BNModel().bootstrap_data(ocean_model_dict,ocean_data_dict,df_ocean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:46.021235Z",
     "start_time": "2021-06-10T02:19:00.755300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Discretise the data\n",
    "lagoon_file_label = \"lagoon\"\n",
    "ocean_file_label = \"ocean\"\n",
    "\n",
    "lagoon_model_dict = BNModel().discretiser(lagoon_model_dict,[])\n",
    "ocean_model_dict = BNModel().discretiser(ocean_model_dict,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:46.096175Z",
     "start_time": "2021-06-10T02:19:46.022236Z"
    }
   },
   "outputs": [],
   "source": [
    "# lagoon_model_dict['variables']['MEI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.387291Z",
     "start_time": "2021-06-10T02:19:46.102171Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lagoon_disc_fig = BNModel().plot_discretiser(lagoon_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.404279Z",
     "start_time": "2021-06-10T02:18:06.256Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ocean_disc_fig = BNModel().plot_discretiser(ocean_model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.711518Z",
     "start_time": "2021-06-10T02:20:53.985216Z"
    }
   },
   "outputs": [],
   "source": [
    "BNModel().save_dataset(lagoon_model_dict,lagoon_file_label)\n",
    "BNModel().save_dataset(ocean_model_dict,ocean_file_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create BN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:21:05.864450Z",
     "start_time": "2021-06-10T02:21:05.152790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the BN\n",
    "lagoon_model_dict = BNModel().create_SM(lagoon_model_dict,lagoon_file_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:21:05.864450Z",
     "start_time": "2021-06-10T02:21:05.152790Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_214/1907274831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mocean_model_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_SM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mocean_model_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mocean_file_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/src/python_classes/BNModel.py\u001b[0m in \u001b[0;36mcreate_SM\u001b[0;34m(self, model_dict, file_label)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Learn the paramters using EM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatching\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;31m# Save the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ocean_model_dict = BNModel().create_SM(ocean_model_dict,ocean_file_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Conditional Probability tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.809434Z",
     "start_time": "2021-06-10T02:20:56.573Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get conditional probs tables\n",
    "BNModel().get_conditional_prob_table(lagoon_model_dict,'TWL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.814431Z",
     "start_time": "2021-06-10T02:20:56.725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get conditional probs tables\n",
    "df_CPT_MSL = BNModel().get_conditional_prob_table(ocean_model_dict,'TWL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.822424Z",
     "start_time": "2021-06-10T02:20:56.885Z"
    }
   },
   "outputs": [],
   "source": [
    "df_CPT_MSL = df_CPT_MSL.loc[['VeryLow','Low','Mid','High','VeryHigh']]\n",
    "\n",
    "plt.pcolor(df_CPT_MSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.827422Z",
     "start_time": "2021-06-10T02:20:57.062Z"
    }
   },
   "outputs": [],
   "source": [
    "df_CPT_MSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.833418Z",
     "start_time": "2021-06-10T02:20:57.381Z"
    }
   },
   "outputs": [],
   "source": [
    "lagoon_evidence_dict = {\n",
    "    'wind_u':[0.05,0.1,0.01,0.01,0.01],\n",
    "    'wind_v':[0.05,0.1,0.01,0.01,0.01,0.01]\n",
    "}\n",
    "\n",
    "ocean_evidence_dict = {\n",
    "    'MSL':[0.05,0.1,0.01,0.01,0.01],\n",
    "    'Tide':[0.05,0.1,0.01,0.01,0.01,0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.838415Z",
     "start_time": "2021-06-10T02:20:57.549Z"
    }
   },
   "outputs": [],
   "source": [
    "lagoon_model_dict = BNModel().add_evidence_to_dict(lagoon_model_dict,lagoon_evidence_dict)\n",
    "ocean_model_dict = BNModel().add_evidence_to_dict(ocean_model_dict,ocean_evidence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update beliefs based on evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.843410Z",
     "start_time": "2021-06-10T02:20:57.946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set evidence and get beliefs\n",
    "lagoon_model_dict = BNModel().update_evidence(lagoon_model_dict)\n",
    "ocean_model_dict = BNModel().update_evidence(ocean_model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.849407Z",
     "start_time": "2021-06-10T02:20:58.285Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up graph\n",
    "graph_lagoon = BNModel().create_BN_graph()\n",
    "\n",
    "# Create nodes of the graph\n",
    "graph_lagoon,lagoon_model_dict = BNModel().create_nodes(graph_lagoon,lagoon_model_dict,0)\n",
    "\n",
    "# Create arcs between nodesb\n",
    "graph_lagoon = BNModel().create_arcs(graph_lagoon,lagoon_model_dict)\n",
    "\n",
    "# Save as dot file\n",
    "graph_lagoon.render(filename='graph_lagoon',format='png')\n",
    "\n",
    "# Plot the graph\n",
    "graph_lagoon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:59.261392Z",
     "start_time": "2021-06-10T02:20:59.150428Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up graph\n",
    "graph_ocean = BNModel().create_BN_graph()\n",
    "\n",
    "# Create nodes of the graph\n",
    "graph_ocean,ocean_model_dict = BNModel().create_nodes(graph_ocean,ocean_model_dict,0)\n",
    "\n",
    "# Create arcs between nodesb\n",
    "graph_ocean = BNModel().create_arcs(graph_ocean,ocean_model_dict)\n",
    "\n",
    "# Save as dot file\n",
    "graph_ocean.render(filename='graph_ocean',format='png')\n",
    "\n",
    "# Plot the graph\n",
    "graph_ocean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.419271Z",
     "start_time": "2021-06-10T02:18:06.285Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### Research picking number of kfolds \n",
    "\n",
    "# lagoon_acc_dict = BNModel().get_accuracies(lagoon_model_dict,\"TWL\")\n",
    "\n",
    "# print(lagoon_acc_dict)\n",
    "\n",
    "# fig, ax = BNModel().confusion_matrix(lagoon_model_dict,\"TWL\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.420270Z",
     "start_time": "2021-06-10T02:18:06.287Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# ocean_acc_dict = BNModel().get_accuracies(ocean_model_dict,\"TWL\")\n",
    "\n",
    "# print(ocean_acc_dict)\n",
    "\n",
    "# fig, ax = BNModel().confusion_matrix(ocean_model_dict,\"TWL\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Lagoon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tide on TWL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSL Priors based on Future Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the distribution of MSL data currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.429266Z",
     "start_time": "2021-06-10T02:18:06.308Z"
    }
   },
   "outputs": [],
   "source": [
    "# def grouping_bins(data):\n",
    "#     grouped_dict = {}\n",
    "\n",
    "#     for bin_name in np.unique(data):\n",
    "#         grouped_dict.update({\n",
    "#             bin_name:len(data[data==bin_name])\n",
    "#         })\n",
    "        \n",
    "#     df_grouped = pd.DataFrame.from_dict(grouped_dict,orient='index')\n",
    "    \n",
    "#     return(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.430264Z",
     "start_time": "2021-06-10T02:18:06.309Z"
    }
   },
   "outputs": [],
   "source": [
    "# def descretiser(bin_edges,bin_names,data_array):\n",
    "    \n",
    "#     discretised_array = np.empty(len(data_array)).astype(str)\n",
    "    \n",
    "#     for lower, upper, bin_name in zip(\n",
    "#         bin_edges[:-1],\n",
    "#         bin_edges[1:],\n",
    "#         bin_names):\n",
    "        \n",
    "#         discretised_array[(data_array>lower)&(data_array<upper)] = bin_name\n",
    "\n",
    "#         # Now Include out of bounds values in the upper and lower bins\n",
    "#         discretised_array[data_array<np.min(bin_edges)] = bin_names[0]\n",
    "#         discretised_array[data_array>np.max(bin_edges)] = bin_names[-1]\n",
    "        \n",
    "#     return(discretised_array)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.431263Z",
     "start_time": "2021-06-10T02:18:06.311Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Bin the MSL data\n",
    "# MSL_data = lagoon_model_dict['variables']['MSL']['training_data'][0]\n",
    "\n",
    "# # Grouping the bins \n",
    "# MSL_bins_grouped = grouping_bins(MSL_data)\n",
    "\n",
    "# # Put the bins in the right order\n",
    "# df_MSL_distribution = MSL_bins_grouped.reindex(lagoon_model_dict['variables']['MSL']['discretisation']['bin_names'])\n",
    "\n",
    "# # Plot the data\n",
    "# plt.plot(df_MSL_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the Global average for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.432263Z",
     "start_time": "2021-06-10T02:18:06.313Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Get the preprocessed data to add to\n",
    "# MSL_training_data_preprocessed = lagoon_model_dict['variables']['MSL']['training_data_preprocessed'][0]\n",
    "# MSL_testing_data_preprocessed = lagoon_model_dict['variables']['MSL']['testing_data_preprocessed'][0]\n",
    "\n",
    "# # Print some stats to get an idea of how adding SLR will affect the data\n",
    "# print(np.min(MSL_training_data_preprocessed))\n",
    "# print(np.max(MSL_training_data_preprocessed))\n",
    "# print(np.mean(MSL_training_data_preprocessed))\n",
    "# print(np.median(MSL_training_data_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.433262Z",
     "start_time": "2021-06-10T02:18:06.315Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Based on a 2 mm/yr rate\n",
    "# mean_SLR_2050 = 2*29/1000\n",
    "# mean_SLR_2100 = 2*59/1000\n",
    "\n",
    "# # Get MSLs adjusted for regional sea level rise\n",
    "# MSL_2050 = MSL_training_data_preprocessed+mean_SLR_2050\n",
    "# MSL_2100 = MSL_training_data_preprocessed+mean_SLR_2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.434261Z",
     "start_time": "2021-06-10T02:18:06.317Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Using the bins from before, discretise the results\n",
    "# bin_edges = lagoon_model_dict['variables']['MSL']['bin_edges'][0]\n",
    "# bin_names = lagoon_model_dict['variables']['MSL']['discretisation']['bin_names']\n",
    "\n",
    "# # Normalise the data\n",
    "# df_MSL_2021 = df_MSL_distribution/np.nansum(df_MSL_distribution)\n",
    "\n",
    "# # Calculated discretised MSL for the scenarios\n",
    "# MSL_2050 = MSL_training_data_preprocessed+mean_SLR_2050\n",
    "# MSL_2050_discretised = descretiser(bin_edges,bin_names,MSL_2050)\n",
    "# df_MSL_2050 = grouping_bins(MSL_2050_discretised)\n",
    "# df_MSL_2050 = df_MSL_2050.reindex(lagoon_model_dict['variables']['MSL']['discretisation']['bin_names'])\n",
    "# df_MSL_2050[df_MSL_2050.isna()] = 0.001\n",
    "# df_MSL_2050 = df_MSL_2050/np.nansum(df_MSL_2050)\n",
    "\n",
    "# # Calculated discretised MSL for the scenarios\n",
    "# MSL_2100 = MSL_training_data_preprocessed+mean_SLR_2100\n",
    "# MSL_2100_discretised = descretiser(bin_edges,bin_names,MSL_2100)\n",
    "# df_MSL_2100 = grouping_bins(MSL_2100_discretised)\n",
    "# df_MSL_2100 = df_MSL_2100.reindex(lagoon_model_dict['variables']['MSL']['discretisation']['bin_names'])\n",
    "# df_MSL_2100[df_MSL_2100.isna()] = 0.001\n",
    "# df_MSL_2100 = df_MSL_2100/np.nansum(df_MSL_2100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set evidence of 2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.435261Z",
     "start_time": "2021-06-10T02:18:06.319Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create dict of evidence\n",
    "# MSL_2050_evidence_dict = {\n",
    "#     'MSL':list(df_MSL_2050[0])\n",
    "# }\n",
    "\n",
    "# # Add evidence for MSL to the model dict\n",
    "# lagoon_model_dict = BNModel().add_evidence_to_dict(lagoon_model_dict,MSL_2050_evidence_dict)\n",
    "# ocean_model_dict = BNModel().add_evidence_to_dict(ocean_model_dict,MSL_2050_evidence_dict)\n",
    "\n",
    "# # Set evidence and get beliefs\n",
    "# lagoon_model_dict = BNModel().update_evidence(lagoon_model_dict)\n",
    "\n",
    "# # TWL posterior 2050\n",
    "# TWL_MSL_2050_posterior_dict = lagoon_model_dict['variables']['TWL_point_110']['resulting_probs'][0]\n",
    "# df_TWL_MSL_2050_posterior = pd.DataFrame.from_dict(TWL_MSL_2050_posterior_dict,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.437260Z",
     "start_time": "2021-06-10T02:18:06.321Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create dict of evidence\n",
    "# MSL_2100_evidence_dict = {\n",
    "#     'MSL':list(df_MSL_2100[0])\n",
    "# }\n",
    "\n",
    "# # Add evidence for MSL to the model dict\n",
    "# lagoon_model_dict = BNModel().add_evidence_to_dict(lagoon_model_dict,MSL_2100_evidence_dict)\n",
    "# ocean_model_dict = BNModel().add_evidence_to_dict(ocean_model_dict,MSL_2100_evidence_dict)\n",
    "\n",
    "# # Set evidence and get beliefs\n",
    "# lagoon_model_dict = BNModel().update_evidence(lagoon_model_dict)\n",
    "\n",
    "# # TWL posterior 2100\n",
    "# TWL_MSL_2100_posterior_dict = lagoon_model_dict['variables']['TWL_point_110']['resulting_probs'][0]\n",
    "# df_TWL_MSL_2100_posterior = pd.DataFrame.from_dict(TWL_MSL_2100_posterior_dict,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.438260Z",
     "start_time": "2021-06-10T02:18:06.323Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # get the original data to compare it to\n",
    "# TWL_training_dict = lagoon_model_dict['variables']['TWL_point_110']['training_data'][0]\n",
    "# df_TWL_training = grouping_bins(TWL_training_dict)\n",
    "# df_TWL_training = df_TWL_training.reindex(lagoon_model_dict['variables']['MSL']['discretisation']['bin_names'])\n",
    "# df_TWL_training[df_TWL_training.isna()] = 0.001\n",
    "# df_TWL_training = df_TWL_training/np.nansum(df_TWL_training)\n",
    "\n",
    "# # Create an extra set of dfs for data as a fraction of original\n",
    "# TWL_MSL_2050 = [(ind,y/x) for ind,x,y in zip(df_TWL_training.index,df_TWL_training[0],df_TWL_MSL_2050_posterior[0])]\n",
    "# TWL_MSL_2100 = [(ind,y/x) for ind,x,y in zip(df_TWL_training.index,df_TWL_training[0],df_TWL_MSL_2100_posterior[0])]\n",
    "# df_TWL_MSL_2050 = pd.DataFrame(TWL_MSL_2050).set_index(0)\n",
    "# df_TWL_MSL_2100 = pd.DataFrame(TWL_MSL_2100).set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.439259Z",
     "start_time": "2021-06-10T02:18:06.325Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Set up the figure\n",
    "# fig = plt.figure(figsize=(10,15))\n",
    "# fig.subplots_adjust(hspace=0.5)\n",
    "# ax1 = plt.subplot2grid((3,1),(0,0))\n",
    "# ax2 = plt.subplot2grid((3,1),(1,0))\n",
    "# ax3 = plt.subplot2grid((3,1),(2,0))\n",
    "\n",
    "# # Plot MSL data and prior distributions\n",
    "# ax1.plot(df_MSL_2021,c='k')\n",
    "# ax1.plot(df_MSL_2050,c='b')\n",
    "# ax1.plot(df_MSL_2100,c='r')\n",
    "\n",
    "# # Plot posterior distributions compared to original data\n",
    "# output_2021, = ax2.plot(df_TWL_training,c='k')\n",
    "# output_2050, = ax2.plot(df_TWL_MSL_2050_posterior,c='b')\n",
    "# output_2100, = ax2.plot(df_TWL_MSL_2100_posterior,c='r')\n",
    "\n",
    "# # Plot posterior distributions as a fraction of the original data\n",
    "# output_2050, = ax3.plot(df_TWL_MSL_2050,c='b')\n",
    "# output_2100, = ax3.plot(df_TWL_MSL_2100,c='r')\n",
    "# ax3_xlims = ax3.get_xlim()\n",
    "# ax3.plot(ax3_xlims,[1,1],c='0.5',ls='--')\n",
    "# ax3.set_xlim(ax3_xlims)\n",
    "\n",
    "# # Format the graph\n",
    "# ax1.set_ylabel('Probability')\n",
    "# ax1.set_xlabel('Mean Sea Level')\n",
    "# ax2.set_ylabel('Probability')\n",
    "# ax2.set_xlabel('Total Water Level')\n",
    "# ax3.set_ylabel('Probability/Probability')\n",
    "# ax3.set_xlabel('Total Water Level')\n",
    "\n",
    "# ax1.legend([output_2021,output_2050,output_2100],['Present','2050','2100'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in some IPCC RSL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.440258Z",
     "start_time": "2021-06-10T02:18:06.327Z"
    }
   },
   "outputs": [],
   "source": [
    "# The IPCC data (Openheimer et al., 2019, IPCC Ch4) is relative to the 1983-2005\n",
    "# Antonio's data is 1993-2017\n",
    "\n",
    "# It's not clear to me right now if the IPCC data is relative to the 1983-2005 value globally averaged, \n",
    "# or if it varies regionally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.442259Z",
     "start_time": "2021-06-10T02:18:06.329Z"
    }
   },
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "# tarawa_lat = 1.333\n",
    "# tarawa_long = 173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.443257Z",
     "start_time": "2021-06-10T02:18:06.331Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load the RSLD from IPCC, chapter\n",
    "# ds_RSLR_2040_rcp26 = xr.open_dataset (r'C:\\Users\\shannonb\\Documents\\Model_and_data\\Dataset\\Additional_data\\Sea_levels\\Data\\Chapter4_SM\\SM4.2\\rsl_26_2040.nc')\n",
    "# ds_RSLR_2040_rcp45 = xr.open_dataset (r'C:\\Users\\shannonb\\Documents\\Model_and_data\\Dataset\\Additional_data\\Sea_levels\\Data\\Chapter4_SM\\SM4.2\\rsl_45_2040.nc')\n",
    "# ds_RSLR_2040_rcp85 = xr.open_dataset (r'C:\\Users\\shannonb\\Documents\\Model_and_data\\Dataset\\Additional_data\\Sea_levels\\Data\\Chapter4_SM\\SM4.2\\rsl_85_2040.nc')\n",
    "\n",
    "# ds_RSLR_2055_rcp26 = xr.open_dataset (r'C:\\Users\\shannonb\\Documents\\Model_and_data\\Dataset\\Additional_data\\Sea_levels\\Data\\Chapter4_SM\\SM4.2\\rsl_26_2055.nc')\n",
    "# ds_RSLR_2055_rcp45 = xr.open_dataset (r'C:\\Users\\shannonb\\Documents\\Model_and_data\\Dataset\\Additional_data\\Sea_levels\\Data\\Chapter4_SM\\SM4.2\\rsl_45_2055.nc')\n",
    "# ds_RSLR_2055_rcp85 = xr.open_dataset (r'C:\\Users\\shannonb\\Documents\\Model_and_data\\Dataset\\Additional_data\\Sea_levels\\Data\\Chapter4_SM\\SM4.2\\rsl_85_2055.nc')\n",
    "\n",
    "# ds_RSLR_2090_rcp26 = xr.open_dataset (r'C:\\Users\\shannonb\\Documents\\Model_and_data\\Dataset\\Additional_data\\Sea_levels\\Data\\Chapter4_SM\\SM4.2\\rsl_26_2090.nc')\n",
    "# ds_RSLR_2090_rcp45 = xr.open_dataset (r'C:\\Users\\shannonb\\Documents\\Model_and_data\\Dataset\\Additional_data\\Sea_levels\\Data\\Chapter4_SM\\SM4.2\\rsl_45_2090.nc')\n",
    "# ds_RSLR_2090_rcp85 = xr.open_dataset (r'C:\\Users\\shannonb\\Documents\\Model_and_data\\Dataset\\Additional_data\\Sea_levels\\Data\\Chapter4_SM\\SM4.2\\rsl_85_2090.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.444258Z",
     "start_time": "2021-06-10T02:18:06.333Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Get the RSLR for Tarawa specifically\n",
    "# RSLR_2040_rcp26 = float(ds_RSLR_2040_rcp26['slr_md'].sel(x=tarawa_long,y=tarawa_lat,method='nearest'))\n",
    "# RSLR_2055_rcp26 = float(ds_RSLR_2055_rcp26['slr_md'].sel(x=tarawa_long,y=tarawa_lat,method='nearest'))\n",
    "# RSLR_2090_rcp26 = float(ds_RSLR_2090_rcp26['slr_md'].sel(x=tarawa_long,y=tarawa_lat,method='nearest'))\n",
    "\n",
    "# RSLR_2040_rcp45 = float(ds_RSLR_2040_rcp45['slr_md'].sel(x=tarawa_long,y=tarawa_lat,method='nearest'))\n",
    "# RSLR_2055_rcp45 = float(ds_RSLR_2055_rcp45['slr_md'].sel(x=tarawa_long,y=tarawa_lat,method='nearest'))\n",
    "# RSLR_2090_rcp45 = float(ds_RSLR_2090_rcp45['slr_md'].sel(x=tarawa_long,y=tarawa_lat,method='nearest'))\n",
    "\n",
    "# RSLR_2040_rcp85 = float(ds_RSLR_2040_rcp85['slr_md'].sel(x=tarawa_long,y=tarawa_lat,method='nearest'))\n",
    "# RSLR_2055_rcp85 = float(ds_RSLR_2055_rcp85['slr_md'].sel(x=tarawa_long,y=tarawa_lat,method='nearest'))\n",
    "# RSLR_2090_rcp85 = float(ds_RSLR_2090_rcp85['slr_md'].sel(x=tarawa_long,y=tarawa_lat,method='nearest'))\n",
    "\n",
    "# #### These values are relative to PI?? If so, I doubt Antonio's numbers are PI---so this needs to be adjusted for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RCP Scenarios to set evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.445255Z",
     "start_time": "2021-06-10T02:18:06.335Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Get MSLs adjusted for regional sea level rise\n",
    "# MSL_2040_rcp26 = MSL_training_data_preprocessed+RSLR_2040_rcp26\n",
    "# MSL_2055_rcp26 = MSL_training_data_preprocessed+RSLR_2055_rcp26\n",
    "# MSL_2090_rcp26 = MSL_training_data_preprocessed+RSLR_2090_rcp26\n",
    "\n",
    "# MSL_2040_rcp45 = MSL_training_data_preprocessed+RSLR_2040_rcp45\n",
    "# MSL_2055_rcp45 = MSL_training_data_preprocessed+RSLR_2055_rcp45\n",
    "# MSL_2090_rcp45 = MSL_training_data_preprocessed+RSLR_2090_rcp45\n",
    "\n",
    "# MSL_2040_rcp85 = MSL_training_data_preprocessed+RSLR_2040_rcp85\n",
    "# MSL_2055_rcp85 = MSL_training_data_preprocessed+RSLR_2055_rcp85\n",
    "# MSL_2090_rcp85 = MSL_training_data_preprocessed+RSLR_2090_rcp85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.447254Z",
     "start_time": "2021-06-10T02:18:06.337Z"
    }
   },
   "outputs": [],
   "source": [
    "# def test_model(evidence_array,model_dict):\n",
    "\n",
    "#     # Adjust MSL for difference between antonio data and IPCC data\n",
    "#     SLR_adjustment = 2*11/1000\n",
    "#     evidence_array -= SLR_adjustment\n",
    "    \n",
    "#     # Dicretise the data\n",
    "#     MSL_discretised = descretiser(bin_edges,bin_names,evidence_array)\n",
    "#     df_MSL = grouping_bins(MSL_discretised)\n",
    "#     df_MSL = df_MSL.reindex(model_dict['variables']['MSL']['discretisation']['bin_names'])\n",
    "#     df_MSL[df_MSL.isna()] = 0.001\n",
    "#     df_MSL = df_MSL/np.nansum(df_MSL)\n",
    "    \n",
    "#     # Create dict of evidence\n",
    "#     MSL_evidence_dict = {\n",
    "#         'MSL':list(df_MSL[0])\n",
    "#     }\n",
    "\n",
    "#     # Add evidence for MSL to the model dict\n",
    "#     model_dict = BNModel().add_evidence_to_dict(model_dict,MSL_evidence_dict)\n",
    "\n",
    "#     # Set evidence and get beliefs\n",
    "#     model_dict = BNModel().update_evidence(model_dict)\n",
    "\n",
    "#     # TWL posterior 2100\n",
    "#     TWL_MSL_posterior_dict = model_dict['variables']['TWL_point_110']['resulting_probs'][0]\n",
    "#     df_TWL_posterior = pd.DataFrame.from_dict(TWL_MSL_posterior_dict,orient='index')\n",
    "    \n",
    "#     return(df_TWL_posterior,df_MSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.448253Z",
     "start_time": "2021-06-10T02:18:06.339Z"
    }
   },
   "outputs": [],
   "source": [
    "# IPCC_posteriors_dict = {}\n",
    "# MSL_prior_dict = {}\n",
    "\n",
    "# MSL_array_list = [MSL_2040_rcp26,MSL_2055_rcp26,MSL_2090_rcp26,MSL_2040_rcp45,MSL_2055_rcp45,MSL_2090_rcp45,MSL_2040_rcp85,MSL_2055_rcp85,MSL_2090_rcp85]\n",
    "\n",
    "# for array, label in zip(MSL_array_list,\n",
    "#                         ['RCP2.6, 2040','RCP2.6, 2055','RCP2.6, 2090','RCP4.5, 2040','RCP4.5, 2055','RCP4.5, 2090','RCP8.5, 2040','RCP8.5, 2055','RCP8.5, 2090']):\n",
    "    \n",
    "#     df_TWL_posterior, df_MSL = test_model(array, lagoon_model_dict)\n",
    "    \n",
    "#     IPCC_posteriors_dict.update({\n",
    "#         label:df_TWL_posterior\n",
    "#     })\n",
    "    \n",
    "#     MSL_prior_dict.update({\n",
    "#         label:df_MSL\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.450252Z",
     "start_time": "2021-06-10T02:18:06.341Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Set up the figure\n",
    "# fig = plt.figure(figsize=(10,15))\n",
    "# fig.subplots_adjust(hspace=0.5)\n",
    "# ax1 = plt.subplot2grid((3,1),(0,0))\n",
    "# ax2 = plt.subplot2grid((3,1),(1,0))\n",
    "# ax3 = plt.subplot2grid((3,1),(2,0))\n",
    "\n",
    "# # # Plot MSL data and prior distributions\n",
    "# # ax1.plot(df_MSL_2021,c='k')\n",
    "# # ax1.plot(df_MSL_2050,c='b')\n",
    "# # ax1.plot(df_MSL_2100,c='r')\n",
    "# for key,df_prior in MSL_prior_dict.items():\n",
    "#     ax1.plot(df_prior)\n",
    "\n",
    "# # # Plot posterior distributions compared to original data\n",
    "# # output_2021, = ax2.plot(df_TWL_training,c='k')\n",
    "# # output_2050, = ax2.plot(df_TWL_MSL_2050_posterior,c='b')\n",
    "# # output_2100, = ax2.plot(df_TWL_MSL_2100_posterior,c='r')\n",
    "# outputs = []\n",
    "\n",
    "# for key,posterior_array in IPCC_posteriors_dict.items():\n",
    "#     output, = ax2.plot(posterior_array)\n",
    "#     outputs.append(output)\n",
    "\n",
    "# # # Plot posterior distributions as a fraction of the original data\n",
    "# # output_2050, = ax3.plot(df_TWL_MSL_2050,c='b')\n",
    "# # output_2100, = ax3.plot(df_TWL_MSL_2100,c='r')\n",
    "# # ax3_xlims = ax3.get_xlim()\n",
    "# # ax3.plot(ax3_xlims,[1,1],c='0.5',ls='--')\n",
    "# # ax3.set_xlim(ax3_xlims)\n",
    "\n",
    "# # Format the graph\n",
    "# ax1.set_ylabel('Probability')\n",
    "# ax1.set_xlabel('Mean Sea Level')\n",
    "# ax2.set_ylabel('Probability')\n",
    "# ax2.set_xlabel('Total Water Level')\n",
    "# ax3.set_ylabel('Probability/Probability')\n",
    "# ax3.set_xlabel('Total Water Level')\n",
    "\n",
    "# ax1.legend(outputs,IPCC_posteriors_dict.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the reef and shoreline profile information\n",
    "df_ocean_profiles = pd.read_csv('/src/Dataset/D8_tarawa_inundation/Profiles_definition_outer_reef_xyxy_processed.txt')\n",
    "df_lagoon_profiles = pd.read_csv('/src/Dataset/D8_tarawa_inundation/Profiles_definition_inner_lagoon_xyxy.txt',delim_whitespace=True,header=None)\n",
    "df_lagoon_profiles.columns = ['reef_long','reef_lat','shore_long','shore_lat','reef_depth']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_locator(value,bin_edges):\n",
    "    '''\n",
    "    function used for determining the index of the appropriate bin for a numerical value.\n",
    "    '''\n",
    "    i=0\n",
    "    for edge_1,edge_2 in zip(bin_edges[:-1],bin_edges[1:]):\n",
    "        if (value>edge_1)&(value<=edge_2):\n",
    "            loc_bin = i\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    if value<=bin_edges[0]:\n",
    "        loc_bin = 0\n",
    "\n",
    "    if value>=bin_edges[-1]:\n",
    "        loc_bin = len(bin_edges)-2\n",
    "\n",
    "    return(loc_bin)\n",
    "\n",
    "def model_location(model_dict,location_details,evidence_dict,variable_list):\n",
    "    \n",
    "    '''\n",
    "    function for adding the location information for one side model to the evidence dictionary\n",
    "    '''\n",
    "    \n",
    "    for variable in variable_list:\n",
    "\n",
    "        bin_edges = model_dict['variables'][variable]['bin_edges'][0]\n",
    "        value = location_details[variable]\n",
    "\n",
    "        var_bin = bin_locator(value,bin_edges)\n",
    "        \n",
    "        evidence_array = [0]*(len(bin_edges)-1)\n",
    "        evidence_array[var_bin] = 1\n",
    "        \n",
    "        evidence_dict.update({\n",
    "            variable:evidence_array\n",
    "        })\n",
    "\n",
    "    # Add evidence to model dict\n",
    "    model_location_dict = BNModel().add_evidence_to_dict(model_dict,evidence_dict)\n",
    "    \n",
    "    # Set evidence and get beliefs\n",
    "    model_location_dict = BNModel().update_evidence(model_location_dict)\n",
    "    \n",
    "    return(model_location_dict)\n",
    "\n",
    "def location_probabilities(evidence_dict,model_dict,variable_list,df_profiles):\n",
    "    '''\n",
    "    \n",
    "    Function for setting evidence and determing probabilties for twl at each point around the island based \n",
    "    on the reef characteristics at each location\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    location_probabilities_dict = {}\n",
    "    \n",
    "    for index,row in df_profiles.iterrows():\n",
    "\n",
    "        model_location_dict = model_location(model_dict,row,evidence_dict,variable_list)\n",
    "        location_probabilities = model_location_dict['variables']['TWL']['resulting_probs'][0]\n",
    "\n",
    "        df_location_probabilities = pd.DataFrame.from_dict(location_probabilities,orient='index')\n",
    "        \n",
    "        largest_cat = df_location_probabilities.idxmax()[0]\n",
    "\n",
    "        location_probabilities_dict.update({\n",
    "            (row.reef_long,row.reef_lat):\\\n",
    "                model_dict['variables']['TWL']['discretisation']['bin_names'].index(largest_cat)\n",
    "        })\n",
    "        \n",
    "    return(location_probabilities_dict)\n",
    "\n",
    "def data2geojson(df):\n",
    "    features = []\n",
    "    insert_features = lambda X: features.append(\n",
    "            geojson.Feature(geometry=geojson.Point((X[\"long\"],\n",
    "                                                    X[\"lat\"])),\n",
    "                            properties=dict(name=X[\"most_likely_twl\"])))\n",
    "    df.apply(insert_features, axis=1)\n",
    "        \n",
    "    return(geojson.FeatureCollection(features))\n",
    "\n",
    "# Load SLR Projections\n",
    "data_location = \"/src/Dataset/D7_MSL_projections/\"\n",
    "file_name = \"distributions_dict\"\n",
    "with open(\"{}{}.json\".format(data_location,file_name), 'r') as fp:\n",
    "    SL_proj_dict = json.load(fp)\n",
    "    \n",
    "def SLR_proj_extractor(SL_proj_dict,AIS_config,rcp,year):\n",
    "    '''\n",
    "    Function for getting SLR projections for a given Antarctic icesheet, rcp and year\n",
    "    Years start as 2020 and go up in lots of 10 until 2150 (2100 for dp16)\n",
    "    '''\n",
    "    SLR_prob_dict = SL_proj_dict[\"('{}', '{}', {})\".format(AIS_config,rcp,year)]\n",
    "    SLR_median_prob = np.max([float(x) for x in list(SLR_prob_dict.keys())])\n",
    "    SLR_median_MSL = float(SLR_prob_dict[str(SLR_median_prob)])/1000 #units is m\n",
    "\n",
    "    return(SLR_median_MSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_figure(\n",
    "    view,tide_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin,proj_time\n",
    "    ):\n",
    "    \n",
    "    ########### get the MSL bin based on slider value\n",
    "    msl_proj = SLR_proj_extractor(SL_proj_dict,'k14','26','{}'.format(proj_time))\n",
    "    ocean_model_dict['variables']['MSL']['bin_edges'][0]\n",
    "    bin_count = bin_locator(msl_proj,ocean_model_dict['variables']['MSL']['bin_edges'][0])\n",
    "    msl_bin = ocean_model_dict['variables']['MSL']['discretisation']['bin_names'][bin_count]\n",
    "    \n",
    "    if view == 'Map':\n",
    "        map_osm = folium.Map(location=[1.448888, 172.991794],zoom_start=11)\n",
    "    elif view == 'Satellite':\n",
    "        token = \"pk.eyJ1Ijoic2hhbm5vbi1iZW5ndHNvbiIsImEiOiJja3F1Y2Q0dHEwMzYwMm9wYmtzYzk2bDZuIn0.5jGMyEiJdmXs1HL7x3ThPw\" # your mapbox token\n",
    "        tileurl = 'https://api.mapbox.com/v4/mapbox.satellite/{z}/{x}/{y}@2x.png?access_token=' + str(token)\n",
    "\n",
    "        map_osm = folium.Map(location=[1.448888, 172.991794], zoom_start=11, tiles=tileurl, attr='Mapbox')\n",
    "        \n",
    "    twl_bin_edges = [round(x,2) for x in ocean_model_dict['variables']['TWL']['bin_edges'][0]]\n",
    "    twl_bins = ocean_model_dict['variables']['TWL']['discretisation']['bin_names']\n",
    "\n",
    "    colours_rgb = matplotlib.pyplot.get_cmap('seismic')(np.arange(0,1+1/len(twl_bins),1/(len(twl_bins)-1)))\n",
    "    colour_hex_dict = {i:rgb2hex(int(255*colours_rgb[i][0]),int(255*colours_rgb[i][1]),int(255*(colours_rgb[i][2]))) for i in np.arange(0,len(twl_bins),1)}\n",
    "    \n",
    "    ################################################\n",
    "    \n",
    "    # Create an empty dictionary for the evidence and populate as you go\n",
    "    ocean_evidence_dict = {}\n",
    "    \n",
    "    for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin],\n",
    "                                ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore']):\n",
    "\n",
    "        ## Set in the evidence dict to be as indicated in the dropdown\n",
    "        bin_index = ocean_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "        # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "        evidence = [0 for x in ocean_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "        evidence[bin_index] = 1\n",
    "        ocean_evidence_dict.update({\n",
    "            var_name:evidence\n",
    "        })\n",
    "    \n",
    "    # Create a list of variables that are location specific to set as evidence in the network\n",
    "    variable_list = ['reef_width','reef_depth','forereef_slope','shore_dir']\n",
    "    \n",
    "    # get the probability dictionary\n",
    "    location_probabilities_dict = location_probabilities(ocean_evidence_dict,ocean_model_dict,variable_list,df_ocean_profiles)\n",
    "    \n",
    "    # Create dataframe to plot\n",
    "    df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "    df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    data_ocean = data2geojson(df_twl_locations)\n",
    "    \n",
    "    colors_hex_points_ocean = [colour_hex_dict[x] for x in df_twl_locations.most_likely_twl]\n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "    # Create an empty dictionary for the evidence and populate as you go\n",
    "    lagoon_evidence_dict = {}\n",
    "    \n",
    "    for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin],\n",
    "                                ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore','wind_u','wind_v']):\n",
    "\n",
    "        ## Set in the evidence dict to be as indicated in the dropdown\n",
    "        bin_index = lagoon_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "        # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "        evidence = [0 for x in lagoon_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "        evidence[bin_index] = 1\n",
    "        lagoon_evidence_dict.update({\n",
    "            var_name:evidence\n",
    "        })\n",
    "    \n",
    "    # Create a list of variables that are location specific to set as evidence in the network\n",
    "    variable_list = []\n",
    "    \n",
    "    # get the probability dictionary\n",
    "    location_probabilities_dict = location_probabilities(lagoon_evidence_dict,lagoon_model_dict,variable_list,df_lagoon_profiles)\n",
    "    \n",
    "    # Create dataframe to plot\n",
    "    df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "    df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    data_lagoon = data2geojson(df_twl_locations)\n",
    "    \n",
    "    colors_hex_points_lagoon = [colour_hex_dict[x] for x in df_twl_locations.most_likely_twl]\n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "    features_list = data_ocean['features']+data_lagoon['features']\n",
    "    \n",
    "    data = data_ocean\n",
    "    data.update({\n",
    "        'features':features_list\n",
    "    })\n",
    "    \n",
    "    colors_hex_points = colors_hex_points_ocean+colors_hex_points_lagoon\n",
    "    \n",
    "    #####################################################################\n",
    "\n",
    "    for feature,color in zip(features_list,colors_hex_points):\n",
    "        feature['properties'] = {'color':color,'weight':1,'markerColor':color,'fillOpacity':1,'fillColor':color}\n",
    "        long,lat = feature['geometry']['coordinates']\n",
    "        \n",
    "        marker = folium.CircleMarker([lat,long],color=color,\n",
    "                                    # popup='<img src={}_{}.png>'.format(int(long*1000),int(lat*1000)),\n",
    "                                   fill_color=color,fill=True,fill_opacity='1',radius=5)\n",
    "        marker.add_to(map_osm)\n",
    "        \n",
    "    twl_bin_edge_labels = ['{} to {} m'.format(\n",
    "        x,y) for x,y in zip(twl_bin_edges[:-1],twl_bin_edges[1:])]\n",
    "        \n",
    "    output_list = []\n",
    "    for rgb_color in colours_rgb:\n",
    "        output = plt.scatter([],[],color=rgb_color)\n",
    "        output_list.append(output)\n",
    "        \n",
    "    legend = plt.legend(output_list,twl_bin_edge_labels,title='Total water level anomaly',fontsize=10)\n",
    "    plt.setp(legend.get_title(),fontsize=12)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.savefig('legend.png')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    url = (\n",
    "        \"legend.png\"\n",
    "    )    \n",
    "    \n",
    "    FloatImage(url, bottom=55, left=55).add_to(map_osm)\n",
    "    \n",
    "    map_osm.save('test.html')\n",
    "        \n",
    "    return(map_osm)\n",
    "    \n",
    "\n",
    "# compile the figure\n",
    "# lagoon_model_dict,ocean_model_dict = initialise_model_dictionaries()\n",
    "tide_bins = ocean_model_dict['variables']['Tide']['discretisation']['bin_names']\n",
    "wave_height_bins = ocean_model_dict['variables']['Hs_offshore']['discretisation']['bin_names']\n",
    "wave_period_bin = ocean_model_dict['variables']['Tm_offshore']['discretisation']['bin_names']\n",
    "wave_direction_bin = ocean_model_dict['variables']['Dir_offshore']['discretisation']['bin_names']\n",
    "wind_u_bin = lagoon_model_dict['variables']['wind_u']['discretisation']['bin_names']\n",
    "wind_v_bin = lagoon_model_dict['variables']['wind_u']['discretisation']['bin_names']\n",
    "# time = list(model_dicts_through_time_dict.keys())  \n",
    "\n",
    "\n",
    "# Create the plot with the widget\n",
    "map_osm = interact(test_figure,\n",
    "                view = widgets.Dropdown(options=['Map','Satellite'],value='Map',description='View type',disabled=False),\n",
    "                tide_bin = widgets.Dropdown(options=tide_bins,value='Mid',description='Tide',disabled=False),\n",
    "                wave_height_bin = widgets.Dropdown(options=wave_height_bins,value='Mid',description='Wave height',disabled=False),\n",
    "                wave_period_bin = widgets.Dropdown(options=wave_period_bin,value='Mid',description='Wave period',disabled=False),\n",
    "                wave_direction_bin = widgets.Dropdown(options=wave_direction_bin,value='NNE',description='Wave direction',disabled=False),\n",
    "                wind_u_bin = widgets.Dropdown(options=wind_u_bin,value='Mid',description='Wind u',disabled=False),\n",
    "                wind_v_bin = widgets.Dropdown(options=wind_v_bin,value='Mid',description='Wind v',disabled=False),\n",
    "                proj_time = widgets.IntSlider(min=2020,max=2150,step=10,value=2020,description='SLR prediction')\n",
    "               )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ocean_model_save_dict = ocean_model_dict.copy()\n",
    "lagoon_model_save_dict = lagoon_model_dict.copy()\n",
    "\n",
    "ocean_model_save_dict.pop('model')\n",
    "lagoon_model_save_dict.pop('model')\n",
    "\n",
    "a_file = open(\"ocean_model.pkl\", \"wb\")\n",
    "pickle.dump(ocean_model_save_dict, a_file)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(\"lagoon_model.pkl\", \"wb\")\n",
    "pickle.dump(lagoon_model_save_dict, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_figure(\n",
    "#     view,tide_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin,proj_time\n",
    "#     ):\n",
    "    \n",
    "#     ########### get the MSL bin based on slider value\n",
    "#     msl_proj = SLR_proj_extractor(SL_proj_dict,'k14','26','{}'.format(proj_time))\n",
    "#     ocean_model_dict['variables']['MSL']['bin_edges'][0]\n",
    "#     bin_count = bin_locator(msl_proj,ocean_model_dict['variables']['MSL']['bin_edges'][0])\n",
    "#     msl_bin = ocean_model_dict['variables']['MSL']['discretisation']['bin_names'][bin_count]\n",
    "    \n",
    "#     if view == 'Map':\n",
    "#         map_osm = folium.Map(location=[1.448888, 172.991794],zoom_start=11)\n",
    "#     elif view == 'Satellite':\n",
    "#         token = \"pk.eyJ1Ijoic2hhbm5vbi1iZW5ndHNvbiIsImEiOiJja3F1Y2Q0dHEwMzYwMm9wYmtzYzk2bDZuIn0.5jGMyEiJdmXs1HL7x3ThPw\" # your mapbox token\n",
    "#         tileurl = 'https://api.mapbox.com/v4/mapbox.satellite/{z}/{x}/{y}@2x.png?access_token=' + str(token)\n",
    "\n",
    "#         map_osm = folium.Map(location=[1.448888, 172.991794], zoom_start=11, tiles=tileurl, attr='Mapbox')\n",
    "        \n",
    "#     twl_bin_edges = [round(x,2) for x in ocean_model_dict['variables']['TWL']['bin_edges'][0]]\n",
    "#     twl_bins = ocean_model_dict['variables']['TWL']['discretisation']['bin_names']\n",
    "\n",
    "#     colours_rgb = matplotlib.pyplot.get_cmap('seismic')(np.arange(0,1+1/len(twl_bins),1/(len(twl_bins)-1)))\n",
    "#     colour_hex_dict = {i:rgb2hex(int(255*colours_rgb[i][0]),int(255*colours_rgb[i][1]),int(255*(colours_rgb[i][2]))) for i in np.arange(0,len(twl_bins),1)}\n",
    "    \n",
    "#     ################################################\n",
    "    \n",
    "#     # Create an empty dictionary for the evidence and populate as you go\n",
    "#     ocean_evidence_dict = {}\n",
    "    \n",
    "#     for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin],\n",
    "#                                 ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore']):\n",
    "\n",
    "#         ## Set in the evidence dict to be as indicated in the dropdown\n",
    "#         bin_index = ocean_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "#         # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "#         evidence = [0 for x in ocean_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "#         evidence[bin_index] = 1\n",
    "#         ocean_evidence_dict.update({\n",
    "#             var_name:evidence\n",
    "#         })\n",
    "    \n",
    "#     # Create a list of variables that are location specific to set as evidence in the network\n",
    "#     variable_list = ['reef_width','reef_depth','forereef_slope','shore_dir']\n",
    "    \n",
    "#     # get the probability dictionary\n",
    "#     location_probabilities_dict = location_probabilities(ocean_evidence_dict,ocean_model_dict,variable_list,df_ocean_profiles)\n",
    "    \n",
    "#     # Get the actual probabilities\n",
    "#     lagoon_twls = [lagoon_model_dict['variables']['TWL']['discretisation']['bin_names'].index(twl)+1\\\n",
    "#          for twl in lagoon_model_dict['variables']['TWL']['testing_data'][0]]\n",
    "    \n",
    "#     print(len(lagoon_twls))\n",
    "#     print(len(location_probabilities_dict.keys()))\n",
    "    \n",
    "#     # Create dataframe to plot\n",
    "#     df_twl_locations =pd.DataFrame({'most_likely_twl':lagoon_twls})\n",
    "#     df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "#     data_ocean = data2geojson(df_twl_locations)\n",
    "    \n",
    "#     colors_hex_points_ocean = [colour_hex_dict[x] for x in df_twl_locations.most_likely_twl]\n",
    "    \n",
    "#     #####################################################################\n",
    "    \n",
    "#     # Create an empty dictionary for the evidence and populate as you go\n",
    "#     lagoon_evidence_dict = {}\n",
    "    \n",
    "#     for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin],\n",
    "#                                 ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore','wind_u','wind_v']):\n",
    "\n",
    "#         ## Set in the evidence dict to be as indicated in the dropdown\n",
    "#         bin_index = lagoon_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "#         # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "#         evidence = [0 for x in lagoon_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "#         evidence[bin_index] = 1\n",
    "#         lagoon_evidence_dict.update({\n",
    "#             var_name:evidence\n",
    "#         })\n",
    "    \n",
    "#     # Create a list of variables that are location specific to set as evidence in the network\n",
    "#     variable_list = []\n",
    "    \n",
    "#     # get the probability dictionary\n",
    "#     location_probabilities_dict = location_probabilities(lagoon_evidence_dict,lagoon_model_dict,variable_list,df_lagoon_profiles)\n",
    "    \n",
    "#     # Create dataframe to plot\n",
    "#     df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "#     df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "#     df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "#     data_lagoon = data2geojson(df_twl_locations)\n",
    "    \n",
    "#     colors_hex_points_lagoon = [colour_hex_dict[x] for x in df_twl_locations.most_likely_twl]\n",
    "    \n",
    "#     #####################################################################\n",
    "    \n",
    "#     features_list = data_ocean['features']+data_lagoon['features']\n",
    "    \n",
    "#     data = data_ocean\n",
    "#     data.update({\n",
    "#         'features':features_list\n",
    "#     })\n",
    "    \n",
    "#     colors_hex_points = colors_hex_points_ocean+colors_hex_points_lagoon\n",
    "    \n",
    "#     #####################################################################\n",
    "\n",
    "#     for feature,color in zip(features_list,colors_hex_points):\n",
    "#         feature['properties'] = {'color':color,'weight':1,'markerColor':color,'fillOpacity':1,'fillColor':color}\n",
    "#         long,lat = feature['geometry']['coordinates']\n",
    "        \n",
    "#         marker = folium.CircleMarker([lat,long],color=color,\n",
    "#                                     # popup='<img src={}_{}.png>'.format(int(long*1000),int(lat*1000)),\n",
    "#                                    fill_color=color,fill=True,fill_opacity='1',radius=5)\n",
    "#         marker.add_to(map_osm)\n",
    "        \n",
    "#     twl_bin_edge_labels = ['{} to {} m'.format(\n",
    "#         x,y) for x,y in zip(twl_bin_edges[:-1],twl_bin_edges[1:])]\n",
    "        \n",
    "#     output_list = []\n",
    "#     for rgb_color in colours_rgb:\n",
    "#         output = plt.scatter([],[],color=rgb_color)\n",
    "#         output_list.append(output)\n",
    "        \n",
    "#     legend = plt.legend(output_list,twl_bin_edge_labels,title='Total water level anomaly',fontsize=10)\n",
    "#     plt.setp(legend.get_title(),fontsize=12)\n",
    "    \n",
    "#     plt.axis('off')\n",
    "#     plt.savefig('legend.png')\n",
    "    \n",
    "#     plt.close()\n",
    "    \n",
    "#     url = (\n",
    "#         \"legend.png\"\n",
    "#     )    \n",
    "    \n",
    "#     FloatImage(url, bottom=55, left=55).add_to(map_osm)\n",
    "    \n",
    "#     map_osm.save('test.html')\n",
    "        \n",
    "#     return(map_osm)\n",
    "    \n",
    "\n",
    "# # compile the figure\n",
    "# # lagoon_model_dict,ocean_model_dict = initialise_model_dictionaries()\n",
    "# tide_bins = ocean_model_dict['variables']['Tide']['discretisation']['bin_names']\n",
    "# wave_height_bins = ocean_model_dict['variables']['Hs_offshore']['discretisation']['bin_names']\n",
    "# wave_period_bin = ocean_model_dict['variables']['Tm_offshore']['discretisation']['bin_names']\n",
    "# wave_direction_bin = ocean_model_dict['variables']['Dir_offshore']['discretisation']['bin_names']\n",
    "# wind_u_bin = lagoon_model_dict['variables']['wind_u']['discretisation']['bin_names']\n",
    "# wind_v_bin = lagoon_model_dict['variables']['wind_u']['discretisation']['bin_names']\n",
    "# # time = list(model_dicts_through_time_dict.keys())  \n",
    "\n",
    "\n",
    "# # Create the plot with the widget\n",
    "# map_osm = interact_manual(test_figure,\n",
    "#                 view = widgets.Dropdown(options=['Map','Satellite'],value='Map',description='View type',disabled=False),\n",
    "#                 tide_bin = widgets.Dropdown(options=tide_bins,value='Mid',description='Tide',disabled=False),\n",
    "#                 wave_height_bin = widgets.Dropdown(options=wave_height_bins,value='Mid',description='Wave height',disabled=False),\n",
    "#                 wave_period_bin = widgets.Dropdown(options=wave_period_bin,value='Mid',description='Wave period',disabled=False),\n",
    "#                 wave_direction_bin = widgets.Dropdown(options=wave_direction_bin,value='NNE',description='Wave direction',disabled=False),\n",
    "#                 wind_u_bin = widgets.Dropdown(options=wind_u_bin,value='Mid',description='Wind u',disabled=False),\n",
    "#                 wind_v_bin = widgets.Dropdown(options=wind_v_bin,value='Mid',description='Wind v',disabled=False),\n",
    "#                 proj_time = widgets.IntSlider(min=2020,max=2150,step=10,value=2020,description='SLR prediction')\n",
    "#                )\n",
    "\n",
    "# map_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagoon_twls = [lagoon_model_dict['variables']['TWL']['discretisation']['bin_names'].index(twl)+1\\\n",
    "     for twl in lagoon_model_dict['variables']['TWL']['testing_data'][0]]\n",
    "df = pd.DataFrame({'most_likely_twl':lagoon_twls})\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagoon_model_dict['variables']['TWL']['testing_data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_locator(value,bin_edges):\n",
    "    '''\n",
    "    function used for determining the index of the appropriate bin for a numerical value.\n",
    "    '''\n",
    "    i=0\n",
    "    for edge_1,edge_2 in zip(bin_edges[:-1],bin_edges[1:]):\n",
    "        if (value>edge_1)&(value<=edge_2):\n",
    "            loc_bin = i\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    if value<=bin_edges[0]:\n",
    "        loc_bin = 0\n",
    "\n",
    "    if value>=bin_edges[-1]:\n",
    "        loc_bin = len(bin_edges)-2\n",
    "\n",
    "    return(loc_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "2344856/I2LE4LVY": {
     "DOI": "10.1029/2019PA003589",
     "author": [
      {
       "family": "Bengtson",
       "given": "Shannon A."
      },
      {
       "family": "Meissner",
       "given": "Katrin J."
      },
      {
       "family": "Menviel",
       "given": "Laurie"
      },
      {
       "family": "A. Sisson",
       "given": "Scott"
      },
      {
       "family": "Wilkin",
       "given": "John"
      }
     ],
     "container-title": "Paleoceanography and Paleoclimatology",
     "container-title-short": "Paleoceanography and Paleoclimatology",
     "id": "2344856/I2LE4LVY",
     "issued": {
      "day": 17,
      "month": 5,
      "year": 2019
     },
     "journalAbbreviation": "Paleoceanography and Paleoclimatology",
     "note": "Citation Key: bengtson2019evaluating",
     "page": "1022-1036",
     "page-first": "1022",
     "title": "Evaluating the extent of North Atlantic Deep Water and the mean Atlantic <sup>13</sup>C from statistical reconstructions",
     "type": "article-journal",
     "volume": "34"
    },
    "2344856/V5HIVSEQ": {
     "DOI": "10.1017/S0263593300020782",
     "URL": "https://www.cambridge.org/core/journals/earth-and-environmental-science-transactions-of-royal-society-of-edinburgh/article/an-alternative-astronomical-calibration-of-the-lower-pleistocene-timescale-based-on-odp-site-677/D02E93BFBF418256AD00642C8A98277C",
     "abstract": "Ocean Drilling Program (ODP) Site 677 provided excellent material for high resolution stable isotope analysis of both benthonic and planktonic foraminifera through the entire Pleistocene and upper Pliocene. The oxygen isotope record is readily correlated with the SPECMAP stack (Imbrie et al. 1984) and with the record from DSDP 607 (Ruddiman et al. 1986) but a significantly better match with orbital models is obtained by departing from the timescale proposed by these authors below Stage 16 (620 000 years). It is the stronger contribution from the precession signal in the record from ODP Site 677 that provides the basis for the revised timescale. Our proposed modification to the timescale would imply that the currently adopted radiometric dates for the MatuyamaBrunhes boundary, the Jaramillo and Olduvai Subchrons and the GaussMatuyama boundary underestimate their true astronomical ages by between 5 and 7%.",
     "accessed": {
      "day": 19,
      "month": 5,
      "year": 2020
     },
     "author": [
      {
       "family": "Shackleton",
       "given": "N. J."
      },
      {
       "family": "Berger",
       "given": "A."
      },
      {
       "family": "Peltier",
       "given": "W. R."
      }
     ],
     "container-title": "Earth and Environmental Science Transactions of The Royal Society of Edinburgh",
     "id": "2344856/V5HIVSEQ",
     "issue": "4",
     "issued": {
      "year": 1990
     },
     "language": "en",
     "note": "citation key: shackleton1990alternative",
     "page": "251-261",
     "page-first": "251",
     "title": "An alternative astronomical calibration of the lower Pleistocene timescale based on ODP Site 677",
     "type": "article-journal",
     "volume": "81"
    }
   }
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "744px",
    "left": "1262px",
    "right": "20px",
    "top": "135px",
    "width": "279px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
